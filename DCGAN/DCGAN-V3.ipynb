{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import midi\n",
    "from pypianoroll import Multitrack, Track \n",
    "import pypianoroll as pypiano\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator \n",
    "#rows, cols and channels\n",
    "#Shape of input array\n",
    "\n",
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        #self.img_rows = 28\n",
    "        #self.img_cols = 28\n",
    "        #self.img_rows = 16\n",
    "        #self.img_cols = 640\n",
    "        self.img_rows = 100\n",
    "        self.img_cols = 128\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        #model.add(Dense(128 * 160 * 4, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        #model.add(Reshape((4, 160, 128)))\n",
    "        model.add(Dense(128 * 32 * 25, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((25, 32, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, X_train, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        #(X_train, _), (_, _) = mnist.load_data()\n",
    "        print(X_train.shape)\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            print(imgs.shape)\n",
    "            print(valid.shape)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "        \n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        print (\"Save begin!\")\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "        print (gen_imgs.shape)\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        str = ('C:\\\\Users\\\\Elisa Pigeon\\\\Pictures\\\\V2\\\\mnist_%d.png' % epoch)\n",
    "        print(str)\n",
    "        fig.savefig(str, format ='png')\n",
    "        plt.close()\n",
    "        \n",
    "    def save_imgs_v2(self, epoch, r, c):\n",
    "        \n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "        print (\"BONJOURRRRR\")\n",
    "        print (gen_imgs.shape)\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        print (gen_imgs.shape)\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        str = ('C:\\\\Users\\\\Elisa Pigeon\\\\Pictures\\\\V2\\\\mnist_HH.png')\n",
    "        print(str)\n",
    "        fig.savefig(str, format ='png')\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = plt.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    result = np.asarray(images)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "def load_gray_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = plt.imread(os.path.join(folder,filename))\n",
    "        img = rgb2gray(img)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    result = np.asarray(images)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_MIDI_folder(folder):\n",
    "    #On va couper le fichier Midi pour avoir un fichier de taille (640, 128)\n",
    "    images = []\n",
    "    for filename in os.listdir(foldername):\n",
    "        multitrack = pypiano.parse(os.path.join(foldername,filename))\n",
    "        list_tracks = multitrack.tracks\n",
    "        #print(len(list_tracks))\n",
    "        for track in list_tracks :\n",
    "            img = track.pianoroll\n",
    "            img = img[0:100]\n",
    "            #print(img.shape)\n",
    "            if img.shape[0] < 100 or img is not None:\n",
    "                images.append(img)\n",
    "    result = np.asarray(images)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldername = \"C:\\\\Users\\\\Elisa Pigeon\\\\Pictures\\\\mid2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_fileName = 'C:\\\\Users\\\\Elisa Pigeon\\\\Pictures\\\\Train'\n",
    "X_train_v2 = load_gray_images_from_folder(Train_fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_v3 = load_images_from_MIDI_folder(foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(100, 128)\n",
      "(21, 16, 640)\n",
      "(196, 100, 128)\n",
      "(196, 100, 128)\n",
      "(20, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_v3.__class__)\n",
    "print(X_train_v3[0].__class__)\n",
    "print(X_train_v3[1].shape)\n",
    "print(X_train_v2.shape)\n",
    "print(X_train_v3.shape)\n",
    "X_train_v3 = np.asarray(X_train_v3)\n",
    "print(X_train_v3.shape)\n",
    "X_train_v4 = X_train_v3[0:20]\n",
    "print(X_train_v4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "it's begin\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_29 (Conv2D)           (None, 50, 64, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 50, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 50, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 25, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 26, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 26, 33, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 26, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 26, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 13, 17, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 13, 17, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 13, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 13, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 13, 17, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 13, 17, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 13, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 13, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 56576)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 56577     \n",
      "=================================================================\n",
      "Total params: 446,209\n",
      "Trainable params: 445,313\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 102400)            10342400  \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 25, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2 (None, 50, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 50, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 50, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 50, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling (None, 100, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 100, 128, 64)      73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 100, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 100, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 100, 128, 1)       577       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 100, 128, 1)       0         \n",
      "=================================================================\n",
      "Total params: 10,565,121\n",
      "Trainable params: 10,564,737\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "(196, 100, 128)\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "0 [D loss: 2.490628, acc.: 35.94%] [G loss: 0.278602]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_0.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1 [D loss: 0.698254, acc.: 67.19%] [G loss: 2.045355]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "2 [D loss: 1.049843, acc.: 40.62%] [G loss: 1.764964]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "3 [D loss: 1.079013, acc.: 39.06%] [G loss: 1.376562]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "4 [D loss: 0.825556, acc.: 51.56%] [G loss: 1.279464]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "5 [D loss: 0.956834, acc.: 43.75%] [G loss: 1.472582]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "6 [D loss: 1.510996, acc.: 18.75%] [G loss: 1.968968]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "7 [D loss: 1.260172, acc.: 25.00%] [G loss: 1.575790]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "8 [D loss: 1.441392, acc.: 18.75%] [G loss: 1.856068]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "9 [D loss: 1.774156, acc.: 6.25%] [G loss: 1.186424]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "10 [D loss: 1.268584, acc.: 28.12%] [G loss: 1.981860]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "11 [D loss: 1.557990, acc.: 14.06%] [G loss: 1.683249]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "12 [D loss: 1.791607, acc.: 6.25%] [G loss: 1.345298]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "13 [D loss: 1.552672, acc.: 7.81%] [G loss: 1.395290]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "14 [D loss: 1.640453, acc.: 9.38%] [G loss: 1.298548]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "15 [D loss: 1.417815, acc.: 9.38%] [G loss: 1.477079]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "16 [D loss: 1.620309, acc.: 10.94%] [G loss: 1.163934]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "17 [D loss: 1.608524, acc.: 15.62%] [G loss: 1.224808]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "18 [D loss: 1.218881, acc.: 15.62%] [G loss: 1.256368]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "19 [D loss: 1.683908, acc.: 10.94%] [G loss: 1.031147]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "20 [D loss: 1.483986, acc.: 12.50%] [G loss: 1.436221]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "21 [D loss: 1.373192, acc.: 21.88%] [G loss: 1.338143]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "22 [D loss: 1.664866, acc.: 6.25%] [G loss: 1.317865]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "23 [D loss: 1.366566, acc.: 20.31%] [G loss: 1.309997]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "24 [D loss: 1.525951, acc.: 17.19%] [G loss: 1.351495]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "25 [D loss: 1.497168, acc.: 20.31%] [G loss: 1.282091]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "26 [D loss: 1.591391, acc.: 9.38%] [G loss: 1.512906]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "27 [D loss: 1.820063, acc.: 3.12%] [G loss: 1.398811]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "28 [D loss: 1.371720, acc.: 15.62%] [G loss: 1.433079]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "29 [D loss: 1.285517, acc.: 15.62%] [G loss: 1.078817]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "30 [D loss: 1.436624, acc.: 17.19%] [G loss: 1.532931]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "31 [D loss: 1.628462, acc.: 10.94%] [G loss: 1.569983]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "32 [D loss: 1.340950, acc.: 26.56%] [G loss: 1.197932]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "33 [D loss: 1.538818, acc.: 12.50%] [G loss: 1.524998]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "34 [D loss: 1.505612, acc.: 14.06%] [G loss: 1.382538]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "35 [D loss: 1.406322, acc.: 15.62%] [G loss: 0.965623]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "36 [D loss: 1.275874, acc.: 23.44%] [G loss: 1.468320]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "37 [D loss: 1.474341, acc.: 15.62%] [G loss: 1.543029]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "38 [D loss: 1.206990, acc.: 25.00%] [G loss: 1.770406]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "39 [D loss: 1.512401, acc.: 21.88%] [G loss: 1.513323]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "40 [D loss: 1.608580, acc.: 14.06%] [G loss: 1.607251]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "41 [D loss: 1.316412, acc.: 20.31%] [G loss: 1.756094]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 [D loss: 1.411635, acc.: 17.19%] [G loss: 1.547834]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "43 [D loss: 1.515289, acc.: 20.31%] [G loss: 1.387667]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "44 [D loss: 1.446350, acc.: 15.62%] [G loss: 1.189380]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "45 [D loss: 1.316490, acc.: 25.00%] [G loss: 1.296756]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "46 [D loss: 1.331833, acc.: 21.88%] [G loss: 1.487851]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "47 [D loss: 1.358721, acc.: 18.75%] [G loss: 1.273098]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "48 [D loss: 1.474847, acc.: 15.62%] [G loss: 1.493390]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "49 [D loss: 1.530576, acc.: 20.31%] [G loss: 1.243439]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "50 [D loss: 1.431399, acc.: 18.75%] [G loss: 1.453743]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_50.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "51 [D loss: 1.356291, acc.: 23.44%] [G loss: 1.532571]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "52 [D loss: 1.436461, acc.: 23.44%] [G loss: 1.109210]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "53 [D loss: 1.255105, acc.: 31.25%] [G loss: 1.323887]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "54 [D loss: 1.230740, acc.: 18.75%] [G loss: 0.957473]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "55 [D loss: 1.545799, acc.: 9.38%] [G loss: 1.191471]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "56 [D loss: 1.280824, acc.: 31.25%] [G loss: 1.315348]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "57 [D loss: 1.461898, acc.: 14.06%] [G loss: 1.421520]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "58 [D loss: 1.779273, acc.: 12.50%] [G loss: 1.112256]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "59 [D loss: 1.441573, acc.: 21.88%] [G loss: 1.137992]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "60 [D loss: 1.364939, acc.: 23.44%] [G loss: 1.603495]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "61 [D loss: 1.484725, acc.: 15.62%] [G loss: 0.986887]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "62 [D loss: 1.348865, acc.: 15.62%] [G loss: 1.475832]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "63 [D loss: 1.759278, acc.: 6.25%] [G loss: 1.280004]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "64 [D loss: 1.487415, acc.: 14.06%] [G loss: 1.321334]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "65 [D loss: 1.779043, acc.: 4.69%] [G loss: 1.038863]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "66 [D loss: 1.446945, acc.: 18.75%] [G loss: 1.237457]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "67 [D loss: 1.474736, acc.: 14.06%] [G loss: 1.409965]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "68 [D loss: 1.356710, acc.: 21.88%] [G loss: 1.907954]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "69 [D loss: 1.186983, acc.: 28.12%] [G loss: 1.922157]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "70 [D loss: 1.268265, acc.: 18.75%] [G loss: 1.487072]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "71 [D loss: 1.608953, acc.: 10.94%] [G loss: 1.219852]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "72 [D loss: 1.574242, acc.: 10.94%] [G loss: 1.274149]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "73 [D loss: 1.292034, acc.: 21.88%] [G loss: 1.274687]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "74 [D loss: 1.565570, acc.: 10.94%] [G loss: 1.399970]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "75 [D loss: 1.337555, acc.: 21.88%] [G loss: 1.171128]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "76 [D loss: 1.322648, acc.: 17.19%] [G loss: 1.325417]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "77 [D loss: 1.568292, acc.: 14.06%] [G loss: 1.219796]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "78 [D loss: 1.424270, acc.: 12.50%] [G loss: 1.076917]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "79 [D loss: 1.356841, acc.: 26.56%] [G loss: 1.379802]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "80 [D loss: 1.287480, acc.: 18.75%] [G loss: 1.313719]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "81 [D loss: 1.325821, acc.: 18.75%] [G loss: 1.323743]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "82 [D loss: 1.458279, acc.: 23.44%] [G loss: 1.236177]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "83 [D loss: 1.359276, acc.: 17.19%] [G loss: 1.269970]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "84 [D loss: 1.276021, acc.: 20.31%] [G loss: 1.320868]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "85 [D loss: 1.511765, acc.: 14.06%] [G loss: 1.147256]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "86 [D loss: 1.499278, acc.: 15.62%] [G loss: 1.159719]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "87 [D loss: 1.313439, acc.: 17.19%] [G loss: 1.518931]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "88 [D loss: 1.697838, acc.: 10.94%] [G loss: 1.290365]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "89 [D loss: 1.561597, acc.: 12.50%] [G loss: 1.315560]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "90 [D loss: 1.631053, acc.: 10.94%] [G loss: 1.369138]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "91 [D loss: 1.329506, acc.: 25.00%] [G loss: 1.504058]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "92 [D loss: 1.321328, acc.: 17.19%] [G loss: 1.037064]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "93 [D loss: 1.660182, acc.: 12.50%] [G loss: 1.233481]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "94 [D loss: 1.313276, acc.: 21.88%] [G loss: 1.442490]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "95 [D loss: 1.430537, acc.: 10.94%] [G loss: 1.189220]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "96 [D loss: 1.507862, acc.: 15.62%] [G loss: 1.220883]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "97 [D loss: 1.394942, acc.: 21.88%] [G loss: 1.471401]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "98 [D loss: 1.111872, acc.: 25.00%] [G loss: 1.096148]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "99 [D loss: 1.095475, acc.: 28.12%] [G loss: 1.193033]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "100 [D loss: 0.898694, acc.: 51.56%] [G loss: 1.217991]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_100.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "101 [D loss: 1.061678, acc.: 32.81%] [G loss: 1.654602]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "102 [D loss: 1.244214, acc.: 18.75%] [G loss: 0.916200]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "103 [D loss: 1.509006, acc.: 18.75%] [G loss: 1.403904]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "104 [D loss: 1.036335, acc.: 31.25%] [G loss: 1.722779]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "105 [D loss: 1.248065, acc.: 21.88%] [G loss: 1.334258]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "106 [D loss: 1.663817, acc.: 10.94%] [G loss: 1.226526]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "107 [D loss: 1.264279, acc.: 23.44%] [G loss: 1.773469]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "108 [D loss: 1.311313, acc.: 18.75%] [G loss: 1.267536]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "109 [D loss: 1.310570, acc.: 18.75%] [G loss: 1.363306]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "110 [D loss: 1.414257, acc.: 21.88%] [G loss: 1.115166]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "111 [D loss: 1.495529, acc.: 21.88%] [G loss: 1.548083]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "112 [D loss: 0.970028, acc.: 37.50%] [G loss: 1.231290]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "113 [D loss: 1.099042, acc.: 37.50%] [G loss: 1.579664]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "114 [D loss: 1.231160, acc.: 17.19%] [G loss: 1.433329]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "115 [D loss: 1.449939, acc.: 15.62%] [G loss: 1.331652]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "116 [D loss: 1.310369, acc.: 25.00%] [G loss: 1.425431]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "117 [D loss: 1.206040, acc.: 29.69%] [G loss: 1.397784]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "118 [D loss: 1.599305, acc.: 10.94%] [G loss: 1.109179]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "119 [D loss: 1.251587, acc.: 23.44%] [G loss: 1.487262]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "120 [D loss: 1.219245, acc.: 26.56%] [G loss: 1.173558]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "121 [D loss: 1.390633, acc.: 20.31%] [G loss: 1.298588]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "122 [D loss: 1.222233, acc.: 35.94%] [G loss: 1.593974]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "123 [D loss: 1.349847, acc.: 25.00%] [G loss: 1.224672]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "124 [D loss: 1.180498, acc.: 34.38%] [G loss: 1.376096]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "125 [D loss: 1.312130, acc.: 18.75%] [G loss: 1.297113]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "126 [D loss: 1.055312, acc.: 32.81%] [G loss: 1.524306]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "127 [D loss: 1.228620, acc.: 26.56%] [G loss: 1.313309]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "128 [D loss: 1.102530, acc.: 31.25%] [G loss: 1.116558]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "129 [D loss: 0.972649, acc.: 34.38%] [G loss: 1.320805]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "130 [D loss: 1.244998, acc.: 20.31%] [G loss: 1.520354]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "131 [D loss: 1.008010, acc.: 31.25%] [G loss: 1.530256]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "132 [D loss: 1.264758, acc.: 21.88%] [G loss: 1.218523]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "133 [D loss: 1.228641, acc.: 29.69%] [G loss: 1.613949]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "134 [D loss: 1.348283, acc.: 18.75%] [G loss: 1.400048]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "135 [D loss: 1.368564, acc.: 23.44%] [G loss: 1.313660]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "136 [D loss: 1.276610, acc.: 21.88%] [G loss: 1.710514]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "137 [D loss: 1.111102, acc.: 35.94%] [G loss: 1.323747]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "138 [D loss: 1.207622, acc.: 26.56%] [G loss: 1.228157]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "139 [D loss: 1.021465, acc.: 40.62%] [G loss: 1.465174]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "140 [D loss: 1.298349, acc.: 23.44%] [G loss: 1.179116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "141 [D loss: 1.178890, acc.: 31.25%] [G loss: 1.593773]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "142 [D loss: 1.463242, acc.: 18.75%] [G loss: 1.351720]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "143 [D loss: 1.466163, acc.: 10.94%] [G loss: 1.312770]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "144 [D loss: 1.116708, acc.: 35.94%] [G loss: 1.004803]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "145 [D loss: 1.379065, acc.: 25.00%] [G loss: 1.257429]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "146 [D loss: 1.070932, acc.: 35.94%] [G loss: 1.471726]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "147 [D loss: 1.212833, acc.: 25.00%] [G loss: 1.558004]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "148 [D loss: 1.165943, acc.: 28.12%] [G loss: 1.612417]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "149 [D loss: 1.556630, acc.: 15.62%] [G loss: 1.433610]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "150 [D loss: 1.242618, acc.: 32.81%] [G loss: 1.287462]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_150.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "151 [D loss: 1.273904, acc.: 21.88%] [G loss: 1.208806]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "152 [D loss: 1.185772, acc.: 25.00%] [G loss: 1.595600]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "153 [D loss: 1.297023, acc.: 26.56%] [G loss: 0.997851]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "154 [D loss: 1.007032, acc.: 34.38%] [G loss: 1.134955]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "155 [D loss: 1.152404, acc.: 29.69%] [G loss: 1.219791]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "156 [D loss: 1.113633, acc.: 26.56%] [G loss: 1.137872]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "157 [D loss: 1.274001, acc.: 15.62%] [G loss: 1.295170]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "158 [D loss: 1.210502, acc.: 21.88%] [G loss: 1.653792]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "159 [D loss: 1.289991, acc.: 21.88%] [G loss: 1.629355]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "160 [D loss: 1.333227, acc.: 21.88%] [G loss: 1.220694]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "161 [D loss: 1.378310, acc.: 23.44%] [G loss: 1.220698]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "162 [D loss: 1.456847, acc.: 12.50%] [G loss: 1.341104]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "163 [D loss: 1.411194, acc.: 18.75%] [G loss: 1.419015]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "164 [D loss: 1.219634, acc.: 26.56%] [G loss: 1.113860]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "165 [D loss: 1.168257, acc.: 31.25%] [G loss: 1.450455]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "166 [D loss: 0.950876, acc.: 39.06%] [G loss: 1.072354]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "167 [D loss: 1.520356, acc.: 21.88%] [G loss: 1.319728]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "168 [D loss: 1.197245, acc.: 18.75%] [G loss: 1.611679]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "169 [D loss: 1.253713, acc.: 26.56%] [G loss: 1.452411]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "170 [D loss: 1.384748, acc.: 20.31%] [G loss: 1.071786]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "171 [D loss: 1.136566, acc.: 28.12%] [G loss: 1.316035]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "172 [D loss: 1.176239, acc.: 25.00%] [G loss: 1.258146]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "173 [D loss: 1.401200, acc.: 17.19%] [G loss: 1.186904]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "174 [D loss: 1.114795, acc.: 32.81%] [G loss: 1.526316]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "175 [D loss: 1.364478, acc.: 23.44%] [G loss: 1.234103]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "176 [D loss: 1.226507, acc.: 25.00%] [G loss: 0.975512]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "177 [D loss: 1.438862, acc.: 18.75%] [G loss: 1.143384]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "178 [D loss: 1.104561, acc.: 32.81%] [G loss: 1.409840]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "179 [D loss: 1.053936, acc.: 37.50%] [G loss: 1.315183]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "180 [D loss: 1.276103, acc.: 17.19%] [G loss: 1.178366]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "181 [D loss: 1.121075, acc.: 34.38%] [G loss: 1.488156]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "182 [D loss: 1.146317, acc.: 32.81%] [G loss: 1.678523]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "183 [D loss: 1.263410, acc.: 14.06%] [G loss: 1.558048]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "184 [D loss: 1.088469, acc.: 39.06%] [G loss: 1.331900]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "185 [D loss: 1.377207, acc.: 20.31%] [G loss: 1.246422]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "186 [D loss: 1.190706, acc.: 32.81%] [G loss: 1.143290]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "187 [D loss: 1.143045, acc.: 26.56%] [G loss: 1.408705]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "188 [D loss: 1.454920, acc.: 17.19%] [G loss: 1.285412]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "189 [D loss: 1.301903, acc.: 23.44%] [G loss: 1.284449]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "190 [D loss: 1.143561, acc.: 25.00%] [G loss: 1.181131]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "191 [D loss: 1.191421, acc.: 26.56%] [G loss: 1.206148]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "192 [D loss: 1.241473, acc.: 23.44%] [G loss: 1.291250]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "193 [D loss: 1.314084, acc.: 28.12%] [G loss: 1.439131]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "194 [D loss: 1.277158, acc.: 20.31%] [G loss: 1.423776]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "195 [D loss: 1.309965, acc.: 18.75%] [G loss: 1.352557]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "196 [D loss: 1.051072, acc.: 37.50%] [G loss: 1.361291]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "197 [D loss: 1.292401, acc.: 15.62%] [G loss: 1.060923]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "198 [D loss: 1.089362, acc.: 28.12%] [G loss: 1.800523]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "199 [D loss: 1.040330, acc.: 35.94%] [G loss: 1.283713]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "200 [D loss: 0.998452, acc.: 34.38%] [G loss: 1.433773]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_200.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "201 [D loss: 1.135911, acc.: 21.88%] [G loss: 1.284733]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "202 [D loss: 1.030837, acc.: 32.81%] [G loss: 1.378637]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "203 [D loss: 1.229317, acc.: 23.44%] [G loss: 1.331460]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "204 [D loss: 1.165025, acc.: 28.12%] [G loss: 1.455961]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "205 [D loss: 1.030758, acc.: 35.94%] [G loss: 1.506057]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "206 [D loss: 1.025736, acc.: 31.25%] [G loss: 1.047042]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "207 [D loss: 0.965134, acc.: 32.81%] [G loss: 1.440454]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "208 [D loss: 1.105751, acc.: 35.94%] [G loss: 1.516141]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "209 [D loss: 1.315408, acc.: 18.75%] [G loss: 1.232856]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "210 [D loss: 1.000503, acc.: 35.94%] [G loss: 1.749764]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "211 [D loss: 1.345476, acc.: 28.12%] [G loss: 1.216773]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "212 [D loss: 0.940518, acc.: 45.31%] [G loss: 0.753722]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "213 [D loss: 0.730417, acc.: 56.25%] [G loss: 0.789325]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "214 [D loss: 0.615198, acc.: 64.06%] [G loss: 0.825116]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "215 [D loss: 0.951359, acc.: 48.44%] [G loss: 1.806740]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "216 [D loss: 1.648305, acc.: 15.62%] [G loss: 1.450619]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "217 [D loss: 1.318008, acc.: 25.00%] [G loss: 1.201105]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "218 [D loss: 1.315145, acc.: 17.19%] [G loss: 1.255824]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "219 [D loss: 1.412951, acc.: 21.88%] [G loss: 1.196493]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "220 [D loss: 1.238487, acc.: 26.56%] [G loss: 1.751863]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "221 [D loss: 1.324065, acc.: 23.44%] [G loss: 1.334316]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "222 [D loss: 1.331694, acc.: 17.19%] [G loss: 1.401570]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "223 [D loss: 1.243716, acc.: 20.31%] [G loss: 1.236597]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "224 [D loss: 1.372926, acc.: 25.00%] [G loss: 1.014360]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "225 [D loss: 1.249023, acc.: 21.88%] [G loss: 1.156117]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "226 [D loss: 1.096918, acc.: 29.69%] [G loss: 1.434410]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "227 [D loss: 1.055358, acc.: 31.25%] [G loss: 1.474052]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "228 [D loss: 1.131571, acc.: 32.81%] [G loss: 1.623466]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "229 [D loss: 1.134142, acc.: 29.69%] [G loss: 1.682721]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "230 [D loss: 1.286705, acc.: 23.44%] [G loss: 1.796915]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "231 [D loss: 1.081450, acc.: 32.81%] [G loss: 1.747356]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "232 [D loss: 1.496325, acc.: 20.31%] [G loss: 1.607815]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "233 [D loss: 1.337298, acc.: 20.31%] [G loss: 1.280293]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "234 [D loss: 1.122561, acc.: 31.25%] [G loss: 1.171908]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "235 [D loss: 0.985819, acc.: 39.06%] [G loss: 1.639963]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "236 [D loss: 0.848423, acc.: 43.75%] [G loss: 1.422288]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "237 [D loss: 1.058236, acc.: 32.81%] [G loss: 1.419545]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "238 [D loss: 1.191069, acc.: 28.12%] [G loss: 1.533133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "239 [D loss: 1.300171, acc.: 17.19%] [G loss: 1.350998]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "240 [D loss: 1.401796, acc.: 15.62%] [G loss: 1.532137]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "241 [D loss: 1.151185, acc.: 32.81%] [G loss: 1.645294]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "242 [D loss: 1.082880, acc.: 34.38%] [G loss: 1.123775]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "243 [D loss: 0.881004, acc.: 53.12%] [G loss: 1.380517]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "244 [D loss: 1.071706, acc.: 34.38%] [G loss: 1.179582]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "245 [D loss: 1.097726, acc.: 34.38%] [G loss: 1.567020]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "246 [D loss: 1.076582, acc.: 40.62%] [G loss: 1.162759]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "247 [D loss: 1.141976, acc.: 29.69%] [G loss: 1.441356]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "248 [D loss: 0.848316, acc.: 50.00%] [G loss: 1.141685]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "249 [D loss: 0.761583, acc.: 59.38%] [G loss: 1.567423]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "250 [D loss: 0.935281, acc.: 45.31%] [G loss: 1.522399]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_250.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "251 [D loss: 0.779776, acc.: 57.81%] [G loss: 1.286818]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "252 [D loss: 0.954097, acc.: 43.75%] [G loss: 1.410624]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "253 [D loss: 1.144091, acc.: 28.12%] [G loss: 1.872756]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "254 [D loss: 1.365417, acc.: 21.88%] [G loss: 1.902976]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "255 [D loss: 1.596372, acc.: 21.88%] [G loss: 1.754897]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "256 [D loss: 1.111313, acc.: 32.81%] [G loss: 1.137600]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "257 [D loss: 0.755022, acc.: 59.38%] [G loss: 0.493941]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "258 [D loss: 0.451849, acc.: 78.12%] [G loss: 0.764659]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "259 [D loss: 0.769460, acc.: 54.69%] [G loss: 1.696361]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "260 [D loss: 1.169277, acc.: 39.06%] [G loss: 2.225796]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "261 [D loss: 1.624885, acc.: 14.06%] [G loss: 1.184367]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "262 [D loss: 1.300464, acc.: 28.12%] [G loss: 1.198567]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "263 [D loss: 1.444003, acc.: 26.56%] [G loss: 1.709715]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "264 [D loss: 1.487486, acc.: 18.75%] [G loss: 1.325764]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "265 [D loss: 1.373436, acc.: 25.00%] [G loss: 1.675951]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "266 [D loss: 1.072567, acc.: 31.25%] [G loss: 1.609816]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "267 [D loss: 1.161928, acc.: 23.44%] [G loss: 1.267337]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "268 [D loss: 0.955457, acc.: 46.88%] [G loss: 1.398495]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "269 [D loss: 0.974707, acc.: 34.38%] [G loss: 1.386542]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "270 [D loss: 0.842852, acc.: 53.12%] [G loss: 1.628713]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "271 [D loss: 0.969095, acc.: 34.38%] [G loss: 1.413580]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "272 [D loss: 0.984889, acc.: 39.06%] [G loss: 1.277633]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "273 [D loss: 1.206980, acc.: 28.12%] [G loss: 1.385611]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "274 [D loss: 1.183679, acc.: 28.12%] [G loss: 1.764769]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "275 [D loss: 1.312250, acc.: 23.44%] [G loss: 1.775604]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "276 [D loss: 1.120958, acc.: 29.69%] [G loss: 1.315329]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "277 [D loss: 0.746257, acc.: 59.38%] [G loss: 1.341330]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "278 [D loss: 0.810930, acc.: 50.00%] [G loss: 1.743610]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "279 [D loss: 0.888774, acc.: 46.88%] [G loss: 2.038566]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "280 [D loss: 1.279729, acc.: 29.69%] [G loss: 1.601453]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "281 [D loss: 1.476739, acc.: 20.31%] [G loss: 1.135885]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "282 [D loss: 1.107814, acc.: 34.38%] [G loss: 1.412785]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "283 [D loss: 1.136816, acc.: 32.81%] [G loss: 1.331193]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "284 [D loss: 1.054525, acc.: 32.81%] [G loss: 1.207757]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "285 [D loss: 0.942747, acc.: 51.56%] [G loss: 1.398788]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "286 [D loss: 0.924291, acc.: 46.88%] [G loss: 1.285218]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "287 [D loss: 0.920743, acc.: 42.19%] [G loss: 1.590453]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "288 [D loss: 0.980018, acc.: 45.31%] [G loss: 1.782061]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "289 [D loss: 0.958182, acc.: 43.75%] [G loss: 1.533522]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "290 [D loss: 0.946059, acc.: 45.31%] [G loss: 1.275020]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "291 [D loss: 1.100061, acc.: 34.38%] [G loss: 1.853751]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "292 [D loss: 1.484479, acc.: 17.19%] [G loss: 1.736748]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "293 [D loss: 1.620861, acc.: 12.50%] [G loss: 1.386986]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "294 [D loss: 1.178553, acc.: 40.62%] [G loss: 0.741010]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "295 [D loss: 0.572353, acc.: 75.00%] [G loss: 0.526786]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "296 [D loss: 0.257619, acc.: 95.31%] [G loss: 0.584307]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "297 [D loss: 0.544208, acc.: 68.75%] [G loss: 1.161411]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "298 [D loss: 0.818205, acc.: 57.81%] [G loss: 2.331522]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "299 [D loss: 1.399845, acc.: 21.88%] [G loss: 1.594231]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "300 [D loss: 1.323606, acc.: 26.56%] [G loss: 1.393709]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_300.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "301 [D loss: 1.216933, acc.: 28.12%] [G loss: 1.329061]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "302 [D loss: 1.077180, acc.: 35.94%] [G loss: 1.357953]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "303 [D loss: 0.913045, acc.: 39.06%] [G loss: 1.707277]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "304 [D loss: 1.380007, acc.: 15.62%] [G loss: 1.320773]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "305 [D loss: 1.073624, acc.: 39.06%] [G loss: 1.523832]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "306 [D loss: 1.038174, acc.: 31.25%] [G loss: 1.389576]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "307 [D loss: 0.972056, acc.: 34.38%] [G loss: 1.280710]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "308 [D loss: 0.823517, acc.: 56.25%] [G loss: 1.739365]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "309 [D loss: 0.884372, acc.: 43.75%] [G loss: 1.336982]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "310 [D loss: 0.870998, acc.: 45.31%] [G loss: 1.625015]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "311 [D loss: 0.920007, acc.: 48.44%] [G loss: 2.204467]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "312 [D loss: 1.139163, acc.: 34.38%] [G loss: 1.393139]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "313 [D loss: 1.350387, acc.: 31.25%] [G loss: 1.683817]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "314 [D loss: 0.869435, acc.: 51.56%] [G loss: 1.672807]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "315 [D loss: 0.606756, acc.: 67.19%] [G loss: 1.630354]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "316 [D loss: 0.487323, acc.: 79.69%] [G loss: 1.953919]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "317 [D loss: 0.639142, acc.: 65.62%] [G loss: 2.040565]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "318 [D loss: 0.794567, acc.: 59.38%] [G loss: 1.692228]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "319 [D loss: 0.963826, acc.: 43.75%] [G loss: 1.919050]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "320 [D loss: 0.811758, acc.: 50.00%] [G loss: 1.921304]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "321 [D loss: 0.752012, acc.: 56.25%] [G loss: 1.274729]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "322 [D loss: 0.841289, acc.: 42.19%] [G loss: 1.492197]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "323 [D loss: 0.952508, acc.: 43.75%] [G loss: 1.714236]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "324 [D loss: 1.218616, acc.: 31.25%] [G loss: 1.744860]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "325 [D loss: 1.955414, acc.: 17.19%] [G loss: 1.971375]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "326 [D loss: 1.452218, acc.: 29.69%] [G loss: 0.976097]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "327 [D loss: 0.861878, acc.: 46.88%] [G loss: 0.836758]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "328 [D loss: 0.432272, acc.: 75.00%] [G loss: 1.322990]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "329 [D loss: 0.611018, acc.: 67.19%] [G loss: 2.155764]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "330 [D loss: 1.064362, acc.: 42.19%] [G loss: 2.362970]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "331 [D loss: 1.438470, acc.: 10.94%] [G loss: 1.628433]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "332 [D loss: 1.045058, acc.: 35.94%] [G loss: 1.542315]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "333 [D loss: 1.056960, acc.: 32.81%] [G loss: 1.430153]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "334 [D loss: 0.931382, acc.: 45.31%] [G loss: 1.845354]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "335 [D loss: 0.764168, acc.: 59.38%] [G loss: 1.613272]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "336 [D loss: 0.679997, acc.: 59.38%] [G loss: 1.640507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "337 [D loss: 0.794793, acc.: 50.00%] [G loss: 1.955419]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "338 [D loss: 0.888185, acc.: 50.00%] [G loss: 1.494869]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "339 [D loss: 0.829872, acc.: 56.25%] [G loss: 1.666065]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "340 [D loss: 0.636547, acc.: 68.75%] [G loss: 2.374464]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "341 [D loss: 0.610722, acc.: 60.94%] [G loss: 2.336910]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "342 [D loss: 0.693191, acc.: 59.38%] [G loss: 2.509003]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "343 [D loss: 1.236967, acc.: 28.12%] [G loss: 2.764903]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "344 [D loss: 1.471606, acc.: 21.88%] [G loss: 1.934967]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "345 [D loss: 0.531254, acc.: 73.44%] [G loss: 1.267133]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "346 [D loss: 0.497539, acc.: 75.00%] [G loss: 0.987861]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "347 [D loss: 0.591614, acc.: 67.19%] [G loss: 2.012743]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "348 [D loss: 1.127282, acc.: 42.19%] [G loss: 2.369413]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "349 [D loss: 1.091770, acc.: 39.06%] [G loss: 1.641246]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "350 [D loss: 1.082690, acc.: 43.75%] [G loss: 1.028334]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_350.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "351 [D loss: 0.787937, acc.: 64.06%] [G loss: 1.321039]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "352 [D loss: 1.097709, acc.: 39.06%] [G loss: 1.948260]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "353 [D loss: 1.507688, acc.: 26.56%] [G loss: 1.356537]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "354 [D loss: 0.725629, acc.: 57.81%] [G loss: 1.845633]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "355 [D loss: 0.858742, acc.: 43.75%] [G loss: 1.845862]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "356 [D loss: 1.092497, acc.: 50.00%] [G loss: 2.500079]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "357 [D loss: 1.191754, acc.: 46.88%] [G loss: 1.746931]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "358 [D loss: 0.942805, acc.: 42.19%] [G loss: 1.788119]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "359 [D loss: 0.817973, acc.: 50.00%] [G loss: 1.620224]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "360 [D loss: 0.702131, acc.: 57.81%] [G loss: 2.262979]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "361 [D loss: 1.354471, acc.: 32.81%] [G loss: 2.317878]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "362 [D loss: 1.622784, acc.: 25.00%] [G loss: 1.970696]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "363 [D loss: 0.754543, acc.: 53.12%] [G loss: 1.995299]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "364 [D loss: 0.697301, acc.: 59.38%] [G loss: 2.343670]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "365 [D loss: 0.523461, acc.: 78.12%] [G loss: 2.682282]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "366 [D loss: 0.693113, acc.: 57.81%] [G loss: 1.996826]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "367 [D loss: 0.670493, acc.: 64.06%] [G loss: 2.048031]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "368 [D loss: 0.640389, acc.: 67.19%] [G loss: 1.868958]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "369 [D loss: 0.399223, acc.: 78.12%] [G loss: 2.036195]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "370 [D loss: 0.475927, acc.: 78.12%] [G loss: 2.150437]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "371 [D loss: 0.485729, acc.: 73.44%] [G loss: 1.544165]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "372 [D loss: 0.589288, acc.: 64.06%] [G loss: 1.710602]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "373 [D loss: 1.028891, acc.: 45.31%] [G loss: 2.304794]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "374 [D loss: 1.158125, acc.: 45.31%] [G loss: 2.440697]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "375 [D loss: 0.794450, acc.: 57.81%] [G loss: 2.478688]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "376 [D loss: 0.991032, acc.: 37.50%] [G loss: 2.743299]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "377 [D loss: 0.948105, acc.: 45.31%] [G loss: 1.844267]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "378 [D loss: 0.895898, acc.: 57.81%] [G loss: 1.297315]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "379 [D loss: 0.511248, acc.: 76.56%] [G loss: 1.458817]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "380 [D loss: 0.616046, acc.: 62.50%] [G loss: 1.782710]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "381 [D loss: 1.092407, acc.: 42.19%] [G loss: 2.601889]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "382 [D loss: 0.893971, acc.: 48.44%] [G loss: 2.080936]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "383 [D loss: 0.747505, acc.: 59.38%] [G loss: 2.300633]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "384 [D loss: 0.694795, acc.: 62.50%] [G loss: 2.363908]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "385 [D loss: 1.192573, acc.: 51.56%] [G loss: 1.699964]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "386 [D loss: 0.441516, acc.: 79.69%] [G loss: 2.654829]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "387 [D loss: 1.087796, acc.: 43.75%] [G loss: 2.043901]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "388 [D loss: 0.702716, acc.: 60.94%] [G loss: 2.032356]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "389 [D loss: 0.570480, acc.: 73.44%] [G loss: 1.435738]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "390 [D loss: 0.510161, acc.: 76.56%] [G loss: 1.566175]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "391 [D loss: 0.513427, acc.: 71.88%] [G loss: 2.628258]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "392 [D loss: 0.894858, acc.: 57.81%] [G loss: 2.541950]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "393 [D loss: 1.009495, acc.: 45.31%] [G loss: 2.746110]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "394 [D loss: 0.894784, acc.: 56.25%] [G loss: 2.538250]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "395 [D loss: 1.036444, acc.: 45.31%] [G loss: 1.645039]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "396 [D loss: 0.673419, acc.: 64.06%] [G loss: 1.986036]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "397 [D loss: 0.849744, acc.: 51.56%] [G loss: 1.994900]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "398 [D loss: 0.918352, acc.: 56.25%] [G loss: 2.423191]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "399 [D loss: 0.511239, acc.: 76.56%] [G loss: 3.090724]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "400 [D loss: 1.386209, acc.: 26.56%] [G loss: 2.228266]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_400.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "401 [D loss: 0.769669, acc.: 53.12%] [G loss: 3.040419]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "402 [D loss: 0.718446, acc.: 65.62%] [G loss: 2.193474]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "403 [D loss: 0.537130, acc.: 68.75%] [G loss: 2.078627]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "404 [D loss: 0.512299, acc.: 68.75%] [G loss: 2.890957]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "405 [D loss: 0.663785, acc.: 62.50%] [G loss: 2.791279]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "406 [D loss: 0.953929, acc.: 50.00%] [G loss: 2.487939]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "407 [D loss: 1.031164, acc.: 45.31%] [G loss: 2.396451]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "408 [D loss: 0.759988, acc.: 53.12%] [G loss: 2.735593]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "409 [D loss: 0.645660, acc.: 71.88%] [G loss: 2.033148]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "410 [D loss: 0.556755, acc.: 68.75%] [G loss: 2.383192]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "411 [D loss: 0.374548, acc.: 85.94%] [G loss: 2.493325]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "412 [D loss: 0.801861, acc.: 50.00%] [G loss: 2.615752]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "413 [D loss: 0.313466, acc.: 95.31%] [G loss: 3.418965]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "414 [D loss: 0.876024, acc.: 48.44%] [G loss: 2.410683]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "415 [D loss: 0.465674, acc.: 75.00%] [G loss: 2.567622]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "416 [D loss: 1.278236, acc.: 28.12%] [G loss: 2.133111]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "417 [D loss: 0.374864, acc.: 87.50%] [G loss: 3.420347]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "418 [D loss: 0.743922, acc.: 59.38%] [G loss: 1.904003]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "419 [D loss: 0.324382, acc.: 85.94%] [G loss: 2.029591]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "420 [D loss: 0.372003, acc.: 89.06%] [G loss: 2.291941]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "421 [D loss: 0.220159, acc.: 93.75%] [G loss: 2.739745]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "422 [D loss: 0.201409, acc.: 98.44%] [G loss: 2.332302]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "423 [D loss: 0.197398, acc.: 92.19%] [G loss: 2.907519]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "424 [D loss: 0.388703, acc.: 79.69%] [G loss: 2.522363]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "425 [D loss: 0.611474, acc.: 67.19%] [G loss: 2.724438]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "426 [D loss: 0.306941, acc.: 90.62%] [G loss: 3.241350]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "427 [D loss: 0.301024, acc.: 90.62%] [G loss: 3.385429]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "428 [D loss: 0.373470, acc.: 84.38%] [G loss: 2.597683]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "429 [D loss: 0.418382, acc.: 82.81%] [G loss: 3.028150]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "430 [D loss: 0.886771, acc.: 59.38%] [G loss: 3.438521]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "431 [D loss: 0.913411, acc.: 48.44%] [G loss: 2.478590]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "432 [D loss: 0.686328, acc.: 62.50%] [G loss: 3.445942]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "433 [D loss: 0.351085, acc.: 84.38%] [G loss: 2.663381]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "434 [D loss: 0.561609, acc.: 71.88%] [G loss: 2.381087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "435 [D loss: 0.791142, acc.: 53.12%] [G loss: 2.535936]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "436 [D loss: 1.473642, acc.: 21.88%] [G loss: 1.955920]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "437 [D loss: 0.835363, acc.: 57.81%] [G loss: 2.746463]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "438 [D loss: 1.271008, acc.: 25.00%] [G loss: 2.306814]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "439 [D loss: 0.588690, acc.: 64.06%] [G loss: 2.362264]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "440 [D loss: 0.763685, acc.: 51.56%] [G loss: 2.499931]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "441 [D loss: 0.522748, acc.: 75.00%] [G loss: 2.544783]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "442 [D loss: 0.465069, acc.: 78.12%] [G loss: 2.504871]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "443 [D loss: 0.513383, acc.: 71.88%] [G loss: 2.910301]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "444 [D loss: 0.378330, acc.: 84.38%] [G loss: 2.757937]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "445 [D loss: 1.123075, acc.: 39.06%] [G loss: 1.892696]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "446 [D loss: 0.256748, acc.: 93.75%] [G loss: 3.075125]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "447 [D loss: 0.462179, acc.: 81.25%] [G loss: 2.227275]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "448 [D loss: 0.833281, acc.: 51.56%] [G loss: 2.717911]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "449 [D loss: 0.800622, acc.: 51.56%] [G loss: 2.920536]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "450 [D loss: 0.514776, acc.: 71.88%] [G loss: 2.819943]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_450.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "451 [D loss: 0.652794, acc.: 70.31%] [G loss: 1.740436]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "452 [D loss: 0.274613, acc.: 92.19%] [G loss: 2.792660]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "453 [D loss: 0.823915, acc.: 56.25%] [G loss: 2.466547]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "454 [D loss: 0.411293, acc.: 79.69%] [G loss: 2.274737]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "455 [D loss: 0.884772, acc.: 45.31%] [G loss: 2.034541]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "456 [D loss: 0.271257, acc.: 92.19%] [G loss: 2.374609]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "457 [D loss: 1.095033, acc.: 37.50%] [G loss: 2.335212]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "458 [D loss: 0.410849, acc.: 81.25%] [G loss: 2.916558]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "459 [D loss: 0.343151, acc.: 85.94%] [G loss: 2.154904]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "460 [D loss: 0.426377, acc.: 78.12%] [G loss: 1.787321]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "461 [D loss: 0.767225, acc.: 60.94%] [G loss: 2.211530]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "462 [D loss: 1.108264, acc.: 35.94%] [G loss: 2.568091]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "463 [D loss: 0.260385, acc.: 89.06%] [G loss: 2.576359]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "464 [D loss: 0.257872, acc.: 93.75%] [G loss: 2.642011]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "465 [D loss: 0.795708, acc.: 57.81%] [G loss: 1.581444]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "466 [D loss: 0.499894, acc.: 78.12%] [G loss: 2.165295]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "467 [D loss: 0.709021, acc.: 59.38%] [G loss: 2.401821]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "468 [D loss: 0.274648, acc.: 92.19%] [G loss: 1.703747]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "469 [D loss: 0.722605, acc.: 64.06%] [G loss: 2.286221]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "470 [D loss: 1.019709, acc.: 32.81%] [G loss: 3.061189]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "471 [D loss: 0.994653, acc.: 43.75%] [G loss: 2.444955]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "472 [D loss: 0.485301, acc.: 76.56%] [G loss: 2.545626]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "473 [D loss: 0.764664, acc.: 60.94%] [G loss: 2.708606]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "474 [D loss: 0.733644, acc.: 60.94%] [G loss: 2.702053]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "475 [D loss: 0.679210, acc.: 60.94%] [G loss: 2.392148]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "476 [D loss: 1.206743, acc.: 37.50%] [G loss: 2.572294]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "477 [D loss: 0.551262, acc.: 70.31%] [G loss: 2.670182]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "478 [D loss: 1.006604, acc.: 35.94%] [G loss: 2.172925]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "479 [D loss: 0.336153, acc.: 85.94%] [G loss: 3.374605]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "480 [D loss: 0.595980, acc.: 65.62%] [G loss: 2.536541]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "481 [D loss: 0.526912, acc.: 78.12%] [G loss: 2.931149]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "482 [D loss: 0.397093, acc.: 78.12%] [G loss: 2.963209]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "483 [D loss: 0.524952, acc.: 73.44%] [G loss: 2.366822]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "484 [D loss: 0.354632, acc.: 85.94%] [G loss: 3.371349]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "485 [D loss: 0.787452, acc.: 56.25%] [G loss: 2.214371]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "486 [D loss: 0.543332, acc.: 75.00%] [G loss: 2.858548]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "487 [D loss: 0.671358, acc.: 65.62%] [G loss: 1.984244]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "488 [D loss: 0.672281, acc.: 57.81%] [G loss: 3.064866]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "489 [D loss: 0.618406, acc.: 67.19%] [G loss: 2.544644]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "490 [D loss: 0.940067, acc.: 46.88%] [G loss: 2.825084]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "491 [D loss: 0.384298, acc.: 82.81%] [G loss: 3.273335]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "492 [D loss: 1.090924, acc.: 34.38%] [G loss: 2.272021]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "493 [D loss: 0.707801, acc.: 57.81%] [G loss: 3.443971]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "494 [D loss: 0.727807, acc.: 50.00%] [G loss: 2.360500]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "495 [D loss: 0.310542, acc.: 90.62%] [G loss: 3.291849]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "496 [D loss: 0.592975, acc.: 68.75%] [G loss: 2.288921]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "497 [D loss: 0.252330, acc.: 93.75%] [G loss: 2.301528]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "498 [D loss: 0.383934, acc.: 84.38%] [G loss: 3.056193]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "499 [D loss: 0.490058, acc.: 70.31%] [G loss: 1.879871]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "500 [D loss: 1.134237, acc.: 37.50%] [G loss: 3.054007]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_500.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "501 [D loss: 0.668733, acc.: 67.19%] [G loss: 2.313125]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "502 [D loss: 0.720366, acc.: 64.06%] [G loss: 2.890337]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "503 [D loss: 0.666562, acc.: 62.50%] [G loss: 2.805544]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "504 [D loss: 1.224259, acc.: 29.69%] [G loss: 2.549824]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "505 [D loss: 0.414287, acc.: 82.81%] [G loss: 2.782923]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "506 [D loss: 0.326415, acc.: 85.94%] [G loss: 2.645101]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "507 [D loss: 0.623908, acc.: 62.50%] [G loss: 2.628400]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "508 [D loss: 0.386569, acc.: 84.38%] [G loss: 2.558128]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "509 [D loss: 0.549526, acc.: 64.06%] [G loss: 2.444150]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "510 [D loss: 0.430443, acc.: 82.81%] [G loss: 3.076404]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "511 [D loss: 0.748773, acc.: 60.94%] [G loss: 2.479409]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "512 [D loss: 0.623189, acc.: 65.62%] [G loss: 2.429946]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "513 [D loss: 0.687986, acc.: 60.94%] [G loss: 2.978738]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "514 [D loss: 0.505921, acc.: 73.44%] [G loss: 3.192415]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "515 [D loss: 0.448677, acc.: 79.69%] [G loss: 2.381848]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "516 [D loss: 0.540080, acc.: 71.88%] [G loss: 2.257510]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "517 [D loss: 0.665329, acc.: 62.50%] [G loss: 2.549142]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "518 [D loss: 0.565261, acc.: 76.56%] [G loss: 2.388998]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "519 [D loss: 0.576179, acc.: 67.19%] [G loss: 2.614155]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "520 [D loss: 0.706207, acc.: 62.50%] [G loss: 2.357658]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "521 [D loss: 0.944231, acc.: 50.00%] [G loss: 1.252309]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "522 [D loss: 0.578703, acc.: 65.62%] [G loss: 3.070351]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "523 [D loss: 0.671366, acc.: 64.06%] [G loss: 1.963728]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "524 [D loss: 0.696667, acc.: 60.94%] [G loss: 2.525516]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "525 [D loss: 0.638222, acc.: 59.38%] [G loss: 3.207786]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "526 [D loss: 1.389661, acc.: 21.88%] [G loss: 2.566431]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "527 [D loss: 0.578709, acc.: 75.00%] [G loss: 3.503510]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "528 [D loss: 0.589606, acc.: 70.31%] [G loss: 2.266249]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "529 [D loss: 0.572648, acc.: 71.88%] [G loss: 2.313900]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "530 [D loss: 0.558071, acc.: 70.31%] [G loss: 3.061021]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "531 [D loss: 0.415348, acc.: 75.00%] [G loss: 2.335124]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "532 [D loss: 0.681036, acc.: 62.50%] [G loss: 2.690981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "533 [D loss: 0.510190, acc.: 75.00%] [G loss: 2.787378]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "534 [D loss: 0.693986, acc.: 67.19%] [G loss: 2.794068]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "535 [D loss: 1.213881, acc.: 37.50%] [G loss: 2.696877]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "536 [D loss: 0.490171, acc.: 75.00%] [G loss: 2.927226]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "537 [D loss: 0.604027, acc.: 73.44%] [G loss: 2.366764]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "538 [D loss: 0.518048, acc.: 79.69%] [G loss: 3.117141]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "539 [D loss: 0.552786, acc.: 70.31%] [G loss: 2.462049]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "540 [D loss: 0.483155, acc.: 76.56%] [G loss: 3.201351]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "541 [D loss: 0.470436, acc.: 75.00%] [G loss: 3.667599]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "542 [D loss: 1.027336, acc.: 48.44%] [G loss: 2.664292]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "543 [D loss: 0.626866, acc.: 59.38%] [G loss: 2.398616]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "544 [D loss: 0.338522, acc.: 84.38%] [G loss: 3.025843]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "545 [D loss: 0.616444, acc.: 67.19%] [G loss: 2.369829]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "546 [D loss: 0.728706, acc.: 68.75%] [G loss: 2.434277]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "547 [D loss: 0.519487, acc.: 65.62%] [G loss: 2.887447]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "548 [D loss: 0.879862, acc.: 53.12%] [G loss: 2.672344]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "549 [D loss: 0.588633, acc.: 65.62%] [G loss: 3.155551]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "550 [D loss: 0.567588, acc.: 64.06%] [G loss: 2.528397]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_550.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "551 [D loss: 0.447113, acc.: 79.69%] [G loss: 3.024741]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "552 [D loss: 0.664258, acc.: 59.38%] [G loss: 2.431059]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "553 [D loss: 0.585006, acc.: 65.62%] [G loss: 2.479969]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "554 [D loss: 0.707717, acc.: 56.25%] [G loss: 3.328632]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "555 [D loss: 0.670274, acc.: 57.81%] [G loss: 1.887259]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "556 [D loss: 0.578161, acc.: 70.31%] [G loss: 3.125868]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "557 [D loss: 0.468054, acc.: 71.88%] [G loss: 2.563854]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "558 [D loss: 0.671727, acc.: 60.94%] [G loss: 2.672959]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "559 [D loss: 0.536406, acc.: 73.44%] [G loss: 3.111634]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "560 [D loss: 0.470493, acc.: 73.44%] [G loss: 2.907429]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "561 [D loss: 0.453392, acc.: 76.56%] [G loss: 2.887024]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "562 [D loss: 0.935248, acc.: 51.56%] [G loss: 3.184067]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "563 [D loss: 0.485823, acc.: 79.69%] [G loss: 2.562589]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "564 [D loss: 0.259008, acc.: 93.75%] [G loss: 2.306100]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "565 [D loss: 0.370735, acc.: 82.81%] [G loss: 2.493147]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "566 [D loss: 0.555844, acc.: 70.31%] [G loss: 3.153018]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "567 [D loss: 0.698570, acc.: 54.69%] [G loss: 2.704902]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "568 [D loss: 0.379452, acc.: 82.81%] [G loss: 3.156677]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "569 [D loss: 0.524192, acc.: 73.44%] [G loss: 1.952590]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "570 [D loss: 0.474279, acc.: 73.44%] [G loss: 3.562194]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "571 [D loss: 1.584237, acc.: 20.31%] [G loss: 2.437119]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "572 [D loss: 0.372179, acc.: 85.94%] [G loss: 2.680641]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "573 [D loss: 0.316648, acc.: 90.62%] [G loss: 2.726172]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "574 [D loss: 0.370606, acc.: 85.94%] [G loss: 3.145672]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "575 [D loss: 0.356226, acc.: 82.81%] [G loss: 3.027345]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "576 [D loss: 0.384226, acc.: 81.25%] [G loss: 2.371409]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "577 [D loss: 0.449434, acc.: 79.69%] [G loss: 2.663145]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "578 [D loss: 0.438764, acc.: 82.81%] [G loss: 2.954307]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "579 [D loss: 1.011352, acc.: 34.38%] [G loss: 2.967451]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "580 [D loss: 0.429837, acc.: 78.12%] [G loss: 3.470546]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "581 [D loss: 0.950682, acc.: 51.56%] [G loss: 2.335978]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "582 [D loss: 0.483504, acc.: 78.12%] [G loss: 2.960223]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "583 [D loss: 0.436069, acc.: 78.12%] [G loss: 2.723567]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "584 [D loss: 0.491000, acc.: 75.00%] [G loss: 2.253356]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "585 [D loss: 0.424169, acc.: 82.81%] [G loss: 2.519988]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "586 [D loss: 0.923943, acc.: 51.56%] [G loss: 3.015663]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "587 [D loss: 0.746181, acc.: 59.38%] [G loss: 2.580971]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "588 [D loss: 0.665484, acc.: 57.81%] [G loss: 2.389449]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "589 [D loss: 0.372930, acc.: 85.94%] [G loss: 3.433644]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "590 [D loss: 0.961429, acc.: 40.62%] [G loss: 2.755920]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "591 [D loss: 0.394485, acc.: 84.38%] [G loss: 3.315373]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "592 [D loss: 1.343704, acc.: 26.56%] [G loss: 3.054751]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "593 [D loss: 0.384342, acc.: 82.81%] [G loss: 3.550854]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "594 [D loss: 0.858617, acc.: 42.19%] [G loss: 2.610809]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "595 [D loss: 0.523017, acc.: 71.88%] [G loss: 3.742734]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "596 [D loss: 1.278556, acc.: 34.38%] [G loss: 2.994580]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "597 [D loss: 0.756788, acc.: 50.00%] [G loss: 3.272932]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "598 [D loss: 0.257750, acc.: 89.06%] [G loss: 2.886050]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "599 [D loss: 0.370119, acc.: 84.38%] [G loss: 2.970375]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "600 [D loss: 0.493699, acc.: 81.25%] [G loss: 3.231890]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_600.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "601 [D loss: 0.476887, acc.: 79.69%] [G loss: 2.767830]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "602 [D loss: 0.430109, acc.: 82.81%] [G loss: 2.390504]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "603 [D loss: 0.592398, acc.: 64.06%] [G loss: 2.893588]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "604 [D loss: 0.596998, acc.: 67.19%] [G loss: 3.632921]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "605 [D loss: 0.498894, acc.: 70.31%] [G loss: 3.442773]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "606 [D loss: 0.521450, acc.: 78.12%] [G loss: 4.118796]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "607 [D loss: 0.404700, acc.: 81.25%] [G loss: 2.818856]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "608 [D loss: 0.445502, acc.: 78.12%] [G loss: 3.387085]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "609 [D loss: 0.187133, acc.: 93.75%] [G loss: 3.890299]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "610 [D loss: 0.225357, acc.: 93.75%] [G loss: 3.138038]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "611 [D loss: 0.280013, acc.: 92.19%] [G loss: 2.385007]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "612 [D loss: 0.567988, acc.: 73.44%] [G loss: 3.242524]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "613 [D loss: 0.642393, acc.: 68.75%] [G loss: 2.791951]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "614 [D loss: 0.738693, acc.: 60.94%] [G loss: 2.674662]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "615 [D loss: 1.464800, acc.: 25.00%] [G loss: 3.485513]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "616 [D loss: 0.538084, acc.: 75.00%] [G loss: 2.959719]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "617 [D loss: 0.694801, acc.: 59.38%] [G loss: 2.792733]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "618 [D loss: 0.182750, acc.: 95.31%] [G loss: 4.107537]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "619 [D loss: 0.224463, acc.: 93.75%] [G loss: 3.740950]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "620 [D loss: 0.080953, acc.: 96.88%] [G loss: 3.886087]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "621 [D loss: 0.140023, acc.: 98.44%] [G loss: 3.458708]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "622 [D loss: 0.127879, acc.: 98.44%] [G loss: 3.285984]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "623 [D loss: 0.270161, acc.: 92.19%] [G loss: 3.241614]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "624 [D loss: 0.179645, acc.: 95.31%] [G loss: 4.000589]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "625 [D loss: 0.759587, acc.: 57.81%] [G loss: 3.162437]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "626 [D loss: 0.337755, acc.: 87.50%] [G loss: 3.253520]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "627 [D loss: 1.405958, acc.: 23.44%] [G loss: 3.003973]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "628 [D loss: 0.524103, acc.: 73.44%] [G loss: 3.735443]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "629 [D loss: 1.044122, acc.: 37.50%] [G loss: 2.554015]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "630 [D loss: 0.255231, acc.: 89.06%] [G loss: 3.818749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "631 [D loss: 0.635880, acc.: 60.94%] [G loss: 3.097534]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "632 [D loss: 0.558365, acc.: 65.62%] [G loss: 3.417170]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "633 [D loss: 0.171668, acc.: 96.88%] [G loss: 3.175332]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "634 [D loss: 0.589747, acc.: 65.62%] [G loss: 2.266478]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "635 [D loss: 0.094124, acc.: 98.44%] [G loss: 3.590472]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "636 [D loss: 0.339397, acc.: 85.94%] [G loss: 3.421825]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "637 [D loss: 0.218611, acc.: 93.75%] [G loss: 3.524096]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "638 [D loss: 0.340072, acc.: 84.38%] [G loss: 3.458527]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "639 [D loss: 0.321397, acc.: 90.62%] [G loss: 2.721638]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "640 [D loss: 0.598042, acc.: 68.75%] [G loss: 3.292420]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "641 [D loss: 1.664675, acc.: 20.31%] [G loss: 3.122345]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "642 [D loss: 0.351302, acc.: 87.50%] [G loss: 3.569224]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "643 [D loss: 0.970857, acc.: 56.25%] [G loss: 2.916372]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "644 [D loss: 0.201748, acc.: 92.19%] [G loss: 3.774379]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "645 [D loss: 0.349526, acc.: 82.81%] [G loss: 3.519473]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "646 [D loss: 0.166187, acc.: 95.31%] [G loss: 3.104320]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "647 [D loss: 0.183288, acc.: 93.75%] [G loss: 3.598929]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "648 [D loss: 0.293685, acc.: 90.62%] [G loss: 3.612650]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "649 [D loss: 0.436294, acc.: 75.00%] [G loss: 3.052459]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "650 [D loss: 0.346089, acc.: 85.94%] [G loss: 3.393731]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_650.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "651 [D loss: 0.380678, acc.: 79.69%] [G loss: 2.867808]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "652 [D loss: 0.525256, acc.: 75.00%] [G loss: 3.792289]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "653 [D loss: 0.581982, acc.: 68.75%] [G loss: 2.921294]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "654 [D loss: 0.530132, acc.: 78.12%] [G loss: 2.892322]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "655 [D loss: 0.315039, acc.: 85.94%] [G loss: 2.868207]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "656 [D loss: 0.357822, acc.: 87.50%] [G loss: 3.480436]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "657 [D loss: 0.356561, acc.: 85.94%] [G loss: 2.699718]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "658 [D loss: 0.359968, acc.: 84.38%] [G loss: 3.271254]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "659 [D loss: 0.360519, acc.: 85.94%] [G loss: 3.171360]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "660 [D loss: 0.344688, acc.: 90.62%] [G loss: 3.511773]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "661 [D loss: 0.337588, acc.: 90.62%] [G loss: 3.204582]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "662 [D loss: 0.380255, acc.: 87.50%] [G loss: 3.189379]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "663 [D loss: 0.382555, acc.: 82.81%] [G loss: 2.966187]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "664 [D loss: 0.644724, acc.: 71.88%] [G loss: 3.917520]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "665 [D loss: 0.542966, acc.: 75.00%] [G loss: 3.226357]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "666 [D loss: 0.377486, acc.: 82.81%] [G loss: 2.065751]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "667 [D loss: 0.368450, acc.: 85.94%] [G loss: 4.161034]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "668 [D loss: 0.257694, acc.: 90.62%] [G loss: 3.314798]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "669 [D loss: 0.268284, acc.: 93.75%] [G loss: 3.350322]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "670 [D loss: 0.729834, acc.: 59.38%] [G loss: 2.962393]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "671 [D loss: 0.615524, acc.: 64.06%] [G loss: 3.827137]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "672 [D loss: 0.223663, acc.: 93.75%] [G loss: 2.908357]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "673 [D loss: 0.538281, acc.: 76.56%] [G loss: 3.465781]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "674 [D loss: 0.383189, acc.: 76.56%] [G loss: 3.080507]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "675 [D loss: 0.549413, acc.: 70.31%] [G loss: 3.959752]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "676 [D loss: 1.088654, acc.: 45.31%] [G loss: 1.686436]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "677 [D loss: 0.419805, acc.: 81.25%] [G loss: 2.756053]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "678 [D loss: 0.360765, acc.: 90.62%] [G loss: 3.878894]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "679 [D loss: 0.452270, acc.: 81.25%] [G loss: 3.043097]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "680 [D loss: 0.870036, acc.: 57.81%] [G loss: 3.061453]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "681 [D loss: 0.527452, acc.: 71.88%] [G loss: 2.804171]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "682 [D loss: 0.528039, acc.: 76.56%] [G loss: 4.150357]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "683 [D loss: 1.048017, acc.: 43.75%] [G loss: 3.519732]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "684 [D loss: 0.349059, acc.: 84.38%] [G loss: 3.327412]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "685 [D loss: 0.784623, acc.: 53.12%] [G loss: 2.818174]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "686 [D loss: 0.314106, acc.: 84.38%] [G loss: 3.209274]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "687 [D loss: 0.717087, acc.: 60.94%] [G loss: 2.872906]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "688 [D loss: 0.270989, acc.: 89.06%] [G loss: 3.088233]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "689 [D loss: 0.348090, acc.: 84.38%] [G loss: 3.194531]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "690 [D loss: 0.186537, acc.: 100.00%] [G loss: 3.144019]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "691 [D loss: 0.418395, acc.: 79.69%] [G loss: 3.514852]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "692 [D loss: 0.257151, acc.: 93.75%] [G loss: 3.103301]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "693 [D loss: 1.242643, acc.: 21.88%] [G loss: 4.198559]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "694 [D loss: 0.810588, acc.: 59.38%] [G loss: 2.433934]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "695 [D loss: 0.478281, acc.: 82.81%] [G loss: 3.676710]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "696 [D loss: 0.483767, acc.: 76.56%] [G loss: 2.886915]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "697 [D loss: 0.656178, acc.: 64.06%] [G loss: 3.790328]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "698 [D loss: 0.328469, acc.: 84.38%] [G loss: 3.624741]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "699 [D loss: 0.234146, acc.: 93.75%] [G loss: 3.440934]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "700 [D loss: 0.818173, acc.: 51.56%] [G loss: 2.942654]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_700.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "701 [D loss: 0.202693, acc.: 95.31%] [G loss: 3.768764]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "702 [D loss: 0.334569, acc.: 84.38%] [G loss: 2.581839]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "703 [D loss: 0.253719, acc.: 92.19%] [G loss: 3.605120]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "704 [D loss: 0.196118, acc.: 95.31%] [G loss: 2.886781]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "705 [D loss: 0.609574, acc.: 70.31%] [G loss: 4.848378]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "706 [D loss: 0.693756, acc.: 62.50%] [G loss: 2.436188]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "707 [D loss: 0.338986, acc.: 92.19%] [G loss: 3.553603]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "708 [D loss: 0.273470, acc.: 90.62%] [G loss: 2.665489]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "709 [D loss: 0.456501, acc.: 71.88%] [G loss: 3.292748]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "710 [D loss: 0.436495, acc.: 81.25%] [G loss: 4.297338]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "711 [D loss: 0.690734, acc.: 53.12%] [G loss: 3.042047]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "712 [D loss: 0.488745, acc.: 76.56%] [G loss: 4.082290]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "713 [D loss: 0.558937, acc.: 70.31%] [G loss: 3.594603]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "714 [D loss: 0.381924, acc.: 82.81%] [G loss: 3.030673]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "715 [D loss: 0.303508, acc.: 85.94%] [G loss: 2.612181]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "716 [D loss: 0.245015, acc.: 90.62%] [G loss: 3.190213]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "717 [D loss: 0.479270, acc.: 73.44%] [G loss: 3.480055]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "718 [D loss: 0.266896, acc.: 90.62%] [G loss: 3.570140]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "719 [D loss: 0.421212, acc.: 82.81%] [G loss: 3.294552]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "720 [D loss: 0.331903, acc.: 90.62%] [G loss: 3.458134]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "721 [D loss: 0.468320, acc.: 79.69%] [G loss: 2.728621]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "722 [D loss: 0.221683, acc.: 92.19%] [G loss: 3.388867]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "723 [D loss: 0.173418, acc.: 98.44%] [G loss: 3.760960]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "724 [D loss: 0.196928, acc.: 95.31%] [G loss: 3.782161]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "725 [D loss: 0.446802, acc.: 79.69%] [G loss: 3.540110]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "726 [D loss: 0.299829, acc.: 84.38%] [G loss: 4.057355]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "727 [D loss: 0.877182, acc.: 50.00%] [G loss: 4.556090]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "728 [D loss: 0.430669, acc.: 79.69%] [G loss: 3.453454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "729 [D loss: 0.299610, acc.: 89.06%] [G loss: 3.786198]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "730 [D loss: 0.332230, acc.: 82.81%] [G loss: 4.294568]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "731 [D loss: 0.284005, acc.: 85.94%] [G loss: 3.289144]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "732 [D loss: 0.986121, acc.: 37.50%] [G loss: 4.118142]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "733 [D loss: 0.343984, acc.: 85.94%] [G loss: 3.721809]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "734 [D loss: 0.387235, acc.: 82.81%] [G loss: 3.470194]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "735 [D loss: 0.174094, acc.: 95.31%] [G loss: 3.727385]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "736 [D loss: 0.389579, acc.: 85.94%] [G loss: 3.953670]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "737 [D loss: 0.464666, acc.: 78.12%] [G loss: 3.352422]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "738 [D loss: 0.478312, acc.: 75.00%] [G loss: 3.058422]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "739 [D loss: 0.169745, acc.: 93.75%] [G loss: 4.080493]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "740 [D loss: 0.219696, acc.: 93.75%] [G loss: 3.312390]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "741 [D loss: 0.198493, acc.: 96.88%] [G loss: 3.610657]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "742 [D loss: 0.343119, acc.: 82.81%] [G loss: 3.894939]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "743 [D loss: 0.739922, acc.: 64.06%] [G loss: 3.169152]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "744 [D loss: 0.178267, acc.: 95.31%] [G loss: 3.750619]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "745 [D loss: 0.203487, acc.: 93.75%] [G loss: 2.717559]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "746 [D loss: 0.359039, acc.: 87.50%] [G loss: 4.220953]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "747 [D loss: 0.485106, acc.: 76.56%] [G loss: 3.464019]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "748 [D loss: 0.362658, acc.: 78.12%] [G loss: 3.966666]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "749 [D loss: 0.549396, acc.: 71.88%] [G loss: 3.484512]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "750 [D loss: 0.683641, acc.: 65.62%] [G loss: 3.562503]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_750.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "751 [D loss: 0.355785, acc.: 87.50%] [G loss: 4.093413]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "752 [D loss: 0.297856, acc.: 89.06%] [G loss: 3.226295]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "753 [D loss: 0.245898, acc.: 87.50%] [G loss: 2.669768]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "754 [D loss: 0.663904, acc.: 65.62%] [G loss: 3.453634]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "755 [D loss: 0.282551, acc.: 90.62%] [G loss: 3.397825]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "756 [D loss: 0.467764, acc.: 82.81%] [G loss: 2.851191]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "757 [D loss: 0.240464, acc.: 92.19%] [G loss: 4.256121]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "758 [D loss: 0.719344, acc.: 60.94%] [G loss: 2.515190]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "759 [D loss: 0.288965, acc.: 87.50%] [G loss: 4.915530]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "760 [D loss: 0.644735, acc.: 64.06%] [G loss: 2.915335]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "761 [D loss: 0.455954, acc.: 79.69%] [G loss: 3.968709]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "762 [D loss: 0.490751, acc.: 73.44%] [G loss: 3.677380]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "763 [D loss: 0.367755, acc.: 81.25%] [G loss: 2.768221]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "764 [D loss: 0.226928, acc.: 92.19%] [G loss: 4.190340]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "765 [D loss: 0.691980, acc.: 67.19%] [G loss: 2.609453]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "766 [D loss: 0.249573, acc.: 92.19%] [G loss: 4.041023]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "767 [D loss: 0.378199, acc.: 85.94%] [G loss: 3.372845]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "768 [D loss: 0.213229, acc.: 95.31%] [G loss: 4.064966]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "769 [D loss: 0.432955, acc.: 81.25%] [G loss: 4.031953]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "770 [D loss: 0.261727, acc.: 93.75%] [G loss: 3.920729]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "771 [D loss: 0.746332, acc.: 57.81%] [G loss: 3.527549]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "772 [D loss: 0.244196, acc.: 93.75%] [G loss: 3.440140]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "773 [D loss: 0.506258, acc.: 75.00%] [G loss: 3.214654]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "774 [D loss: 0.310495, acc.: 84.38%] [G loss: 3.306454]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "775 [D loss: 0.510392, acc.: 71.88%] [G loss: 4.054533]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "776 [D loss: 0.183951, acc.: 90.62%] [G loss: 3.466121]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "777 [D loss: 0.759341, acc.: 59.38%] [G loss: 4.788553]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "778 [D loss: 0.296733, acc.: 85.94%] [G loss: 4.543424]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "779 [D loss: 1.282501, acc.: 37.50%] [G loss: 3.395690]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "780 [D loss: 0.320612, acc.: 84.38%] [G loss: 3.433885]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "781 [D loss: 0.688434, acc.: 60.94%] [G loss: 3.271791]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "782 [D loss: 0.220397, acc.: 90.62%] [G loss: 3.498455]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "783 [D loss: 1.570739, acc.: 21.88%] [G loss: 3.614753]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "784 [D loss: 0.170705, acc.: 96.88%] [G loss: 4.273911]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "785 [D loss: 0.519757, acc.: 73.44%] [G loss: 4.004685]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "786 [D loss: 0.352024, acc.: 78.12%] [G loss: 3.768797]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "787 [D loss: 0.581620, acc.: 70.31%] [G loss: 2.744813]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "788 [D loss: 0.520174, acc.: 70.31%] [G loss: 4.455150]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "789 [D loss: 0.281042, acc.: 92.19%] [G loss: 3.240223]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "790 [D loss: 0.120234, acc.: 96.88%] [G loss: 3.241838]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "791 [D loss: 0.127833, acc.: 98.44%] [G loss: 2.869163]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "792 [D loss: 0.303355, acc.: 89.06%] [G loss: 4.470488]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "793 [D loss: 0.461144, acc.: 75.00%] [G loss: 2.965015]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "794 [D loss: 0.198933, acc.: 93.75%] [G loss: 3.433118]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "795 [D loss: 0.141559, acc.: 95.31%] [G loss: 3.122487]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "796 [D loss: 0.443547, acc.: 82.81%] [G loss: 3.384685]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "797 [D loss: 0.221139, acc.: 92.19%] [G loss: 2.075666]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "798 [D loss: 0.342730, acc.: 90.62%] [G loss: 2.771544]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "799 [D loss: 0.214237, acc.: 93.75%] [G loss: 4.780395]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "800 [D loss: 0.880485, acc.: 48.44%] [G loss: 3.648229]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_800.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "801 [D loss: 0.199552, acc.: 95.31%] [G loss: 3.500469]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "802 [D loss: 0.448181, acc.: 75.00%] [G loss: 3.167614]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "803 [D loss: 0.334329, acc.: 81.25%] [G loss: 3.591060]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "804 [D loss: 0.311492, acc.: 87.50%] [G loss: 3.218900]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "805 [D loss: 0.370932, acc.: 81.25%] [G loss: 4.765067]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "806 [D loss: 0.364129, acc.: 85.94%] [G loss: 3.578055]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "807 [D loss: 0.378900, acc.: 81.25%] [G loss: 3.342294]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "808 [D loss: 0.230534, acc.: 92.19%] [G loss: 4.318470]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "809 [D loss: 0.160969, acc.: 96.88%] [G loss: 3.272910]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "810 [D loss: 0.318707, acc.: 85.94%] [G loss: 4.131353]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "811 [D loss: 0.428147, acc.: 81.25%] [G loss: 3.102825]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "812 [D loss: 0.341892, acc.: 85.94%] [G loss: 4.316179]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "813 [D loss: 0.215562, acc.: 95.31%] [G loss: 3.737644]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "814 [D loss: 0.195452, acc.: 92.19%] [G loss: 2.975110]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "815 [D loss: 1.104445, acc.: 48.44%] [G loss: 3.393402]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "816 [D loss: 0.193520, acc.: 93.75%] [G loss: 4.773185]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "817 [D loss: 0.749070, acc.: 57.81%] [G loss: 3.172627]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "818 [D loss: 0.335558, acc.: 79.69%] [G loss: 4.816050]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "819 [D loss: 0.418208, acc.: 82.81%] [G loss: 3.213199]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "820 [D loss: 0.647963, acc.: 65.62%] [G loss: 4.718143]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "821 [D loss: 0.364718, acc.: 84.38%] [G loss: 3.735042]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "822 [D loss: 0.773782, acc.: 62.50%] [G loss: 3.640491]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "823 [D loss: 0.294632, acc.: 87.50%] [G loss: 3.483068]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "824 [D loss: 0.296627, acc.: 87.50%] [G loss: 4.430939]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "825 [D loss: 0.302049, acc.: 90.62%] [G loss: 2.994732]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "826 [D loss: 0.285871, acc.: 89.06%] [G loss: 2.578666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "827 [D loss: 0.225951, acc.: 93.75%] [G loss: 4.741753]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "828 [D loss: 0.395859, acc.: 81.25%] [G loss: 2.627734]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "829 [D loss: 0.625666, acc.: 65.62%] [G loss: 5.733983]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "830 [D loss: 0.251205, acc.: 87.50%] [G loss: 3.456762]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "831 [D loss: 0.222407, acc.: 93.75%] [G loss: 3.976916]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "832 [D loss: 0.350362, acc.: 81.25%] [G loss: 3.234739]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "833 [D loss: 0.346720, acc.: 85.94%] [G loss: 3.878217]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "834 [D loss: 0.198002, acc.: 93.75%] [G loss: 4.549619]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "835 [D loss: 0.363280, acc.: 84.38%] [G loss: 2.805987]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "836 [D loss: 0.726765, acc.: 48.44%] [G loss: 3.629917]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "837 [D loss: 0.148737, acc.: 98.44%] [G loss: 4.145425]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "838 [D loss: 0.300169, acc.: 85.94%] [G loss: 3.282639]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "839 [D loss: 0.371914, acc.: 79.69%] [G loss: 3.482727]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "840 [D loss: 0.180014, acc.: 95.31%] [G loss: 3.232108]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "841 [D loss: 0.411102, acc.: 81.25%] [G loss: 4.017949]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "842 [D loss: 0.234581, acc.: 93.75%] [G loss: 3.694800]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "843 [D loss: 0.196135, acc.: 96.88%] [G loss: 3.467697]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "844 [D loss: 0.312273, acc.: 87.50%] [G loss: 2.798490]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "845 [D loss: 0.268027, acc.: 87.50%] [G loss: 3.893669]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "846 [D loss: 0.232058, acc.: 89.06%] [G loss: 2.776350]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "847 [D loss: 0.143483, acc.: 98.44%] [G loss: 3.018932]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "848 [D loss: 0.401995, acc.: 75.00%] [G loss: 4.106558]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "849 [D loss: 0.276862, acc.: 85.94%] [G loss: 3.488422]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "850 [D loss: 0.625032, acc.: 70.31%] [G loss: 3.710154]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_850.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "851 [D loss: 0.137422, acc.: 93.75%] [G loss: 5.216849]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "852 [D loss: 0.712756, acc.: 60.94%] [G loss: 2.465589]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "853 [D loss: 0.298355, acc.: 87.50%] [G loss: 4.990554]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "854 [D loss: 0.828627, acc.: 64.06%] [G loss: 2.326783]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "855 [D loss: 0.176791, acc.: 95.31%] [G loss: 3.345618]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "856 [D loss: 0.278198, acc.: 92.19%] [G loss: 3.561516]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "857 [D loss: 0.443204, acc.: 79.69%] [G loss: 4.390431]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "858 [D loss: 0.204701, acc.: 90.62%] [G loss: 3.867028]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "859 [D loss: 0.429668, acc.: 76.56%] [G loss: 4.904211]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "860 [D loss: 1.554274, acc.: 28.12%] [G loss: 4.671154]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "861 [D loss: 0.466795, acc.: 81.25%] [G loss: 3.978369]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "862 [D loss: 0.328426, acc.: 87.50%] [G loss: 2.975453]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "863 [D loss: 0.318881, acc.: 85.94%] [G loss: 3.145882]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "864 [D loss: 0.095208, acc.: 98.44%] [G loss: 4.235001]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "865 [D loss: 0.480916, acc.: 75.00%] [G loss: 3.720536]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "866 [D loss: 0.361354, acc.: 81.25%] [G loss: 2.503300]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "867 [D loss: 0.196954, acc.: 93.75%] [G loss: 3.357115]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "868 [D loss: 0.271257, acc.: 90.62%] [G loss: 3.721328]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "869 [D loss: 0.409801, acc.: 81.25%] [G loss: 2.494160]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "870 [D loss: 0.167226, acc.: 95.31%] [G loss: 3.369536]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "871 [D loss: 0.286148, acc.: 90.62%] [G loss: 3.156381]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "872 [D loss: 0.316961, acc.: 87.50%] [G loss: 2.929357]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "873 [D loss: 0.699169, acc.: 59.38%] [G loss: 4.043206]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "874 [D loss: 0.103950, acc.: 96.88%] [G loss: 3.884699]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "875 [D loss: 0.305145, acc.: 92.19%] [G loss: 3.166218]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "876 [D loss: 0.347622, acc.: 81.25%] [G loss: 3.005943]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "877 [D loss: 0.187441, acc.: 95.31%] [G loss: 4.082269]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "878 [D loss: 0.340316, acc.: 81.25%] [G loss: 4.518296]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "879 [D loss: 0.543764, acc.: 67.19%] [G loss: 3.556010]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "880 [D loss: 0.150948, acc.: 96.88%] [G loss: 4.638836]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "881 [D loss: 0.327525, acc.: 82.81%] [G loss: 4.537054]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "882 [D loss: 0.313801, acc.: 85.94%] [G loss: 3.331027]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "883 [D loss: 0.195648, acc.: 92.19%] [G loss: 3.895848]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "884 [D loss: 0.225456, acc.: 92.19%] [G loss: 4.293133]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "885 [D loss: 0.298482, acc.: 87.50%] [G loss: 3.791350]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "886 [D loss: 0.341849, acc.: 85.94%] [G loss: 3.405863]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "887 [D loss: 0.663800, acc.: 60.94%] [G loss: 3.905979]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "888 [D loss: 0.144265, acc.: 100.00%] [G loss: 5.003781]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "889 [D loss: 0.226440, acc.: 92.19%] [G loss: 3.877369]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "890 [D loss: 0.299742, acc.: 89.06%] [G loss: 3.707614]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "891 [D loss: 0.199224, acc.: 95.31%] [G loss: 3.526329]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "892 [D loss: 0.657473, acc.: 70.31%] [G loss: 3.998254]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "893 [D loss: 0.767892, acc.: 62.50%] [G loss: 3.685248]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "894 [D loss: 0.180829, acc.: 93.75%] [G loss: 4.874109]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "895 [D loss: 0.347892, acc.: 84.38%] [G loss: 3.442012]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "896 [D loss: 0.280229, acc.: 90.62%] [G loss: 3.893024]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "897 [D loss: 0.150886, acc.: 98.44%] [G loss: 3.558036]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "898 [D loss: 0.297879, acc.: 89.06%] [G loss: 3.863195]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "899 [D loss: 0.059349, acc.: 100.00%] [G loss: 4.596286]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "900 [D loss: 0.309557, acc.: 85.94%] [G loss: 3.110474]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_900.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "901 [D loss: 0.284480, acc.: 84.38%] [G loss: 5.641095]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "902 [D loss: 0.345100, acc.: 84.38%] [G loss: 3.784183]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "903 [D loss: 0.136117, acc.: 96.88%] [G loss: 3.971745]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "904 [D loss: 0.270974, acc.: 87.50%] [G loss: 3.562846]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "905 [D loss: 0.219144, acc.: 92.19%] [G loss: 3.638835]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "906 [D loss: 0.057973, acc.: 98.44%] [G loss: 5.372650]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "907 [D loss: 0.882235, acc.: 54.69%] [G loss: 4.171816]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "908 [D loss: 0.168574, acc.: 95.31%] [G loss: 4.347289]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "909 [D loss: 0.315226, acc.: 85.94%] [G loss: 2.804487]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "910 [D loss: 0.488045, acc.: 76.56%] [G loss: 4.300440]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "911 [D loss: 0.121566, acc.: 96.88%] [G loss: 4.877651]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "912 [D loss: 0.490724, acc.: 78.12%] [G loss: 3.760954]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "913 [D loss: 0.120903, acc.: 96.88%] [G loss: 4.111595]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "914 [D loss: 0.217500, acc.: 92.19%] [G loss: 3.510140]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "915 [D loss: 0.418663, acc.: 73.44%] [G loss: 3.658510]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "916 [D loss: 0.457078, acc.: 78.12%] [G loss: 4.655455]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "917 [D loss: 0.305563, acc.: 87.50%] [G loss: 3.514532]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "918 [D loss: 0.277272, acc.: 90.62%] [G loss: 4.626816]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "919 [D loss: 0.229902, acc.: 90.62%] [G loss: 4.050708]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "920 [D loss: 0.239017, acc.: 92.19%] [G loss: 3.434045]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "921 [D loss: 0.463090, acc.: 78.12%] [G loss: 3.916999]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "922 [D loss: 0.256682, acc.: 89.06%] [G loss: 3.777464]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "923 [D loss: 0.116052, acc.: 98.44%] [G loss: 4.320177]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924 [D loss: 0.171653, acc.: 95.31%] [G loss: 3.735568]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "925 [D loss: 0.130804, acc.: 100.00%] [G loss: 4.313960]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "926 [D loss: 0.175363, acc.: 95.31%] [G loss: 2.995294]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "927 [D loss: 0.170109, acc.: 95.31%] [G loss: 4.043586]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "928 [D loss: 0.221462, acc.: 90.62%] [G loss: 4.887734]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "929 [D loss: 0.231203, acc.: 93.75%] [G loss: 4.103959]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "930 [D loss: 0.593018, acc.: 76.56%] [G loss: 3.132906]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "931 [D loss: 0.658183, acc.: 65.62%] [G loss: 6.110847]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "932 [D loss: 0.397788, acc.: 82.81%] [G loss: 2.484459]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "933 [D loss: 0.863889, acc.: 57.81%] [G loss: 5.958950]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "934 [D loss: 0.473125, acc.: 78.12%] [G loss: 3.675823]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "935 [D loss: 1.108060, acc.: 50.00%] [G loss: 5.403621]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "936 [D loss: 0.554035, acc.: 70.31%] [G loss: 3.567173]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "937 [D loss: 0.229409, acc.: 89.06%] [G loss: 4.613345]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "938 [D loss: 0.068237, acc.: 98.44%] [G loss: 4.825372]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "939 [D loss: 0.682829, acc.: 64.06%] [G loss: 3.269032]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "940 [D loss: 0.438871, acc.: 82.81%] [G loss: 3.975157]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "941 [D loss: 1.399798, acc.: 34.38%] [G loss: 4.270998]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "942 [D loss: 0.303111, acc.: 90.62%] [G loss: 4.167678]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "943 [D loss: 0.235653, acc.: 92.19%] [G loss: 2.825572]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "944 [D loss: 0.352358, acc.: 81.25%] [G loss: 3.562077]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "945 [D loss: 0.256748, acc.: 89.06%] [G loss: 3.049930]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "946 [D loss: 0.201299, acc.: 93.75%] [G loss: 4.046177]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "947 [D loss: 1.228530, acc.: 35.94%] [G loss: 2.957521]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "948 [D loss: 0.134990, acc.: 93.75%] [G loss: 4.771829]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "949 [D loss: 0.828086, acc.: 56.25%] [G loss: 3.016060]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "950 [D loss: 0.130953, acc.: 95.31%] [G loss: 4.071282]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_950.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "951 [D loss: 0.331855, acc.: 85.94%] [G loss: 3.561531]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "952 [D loss: 0.284107, acc.: 87.50%] [G loss: 4.994298]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "953 [D loss: 0.343416, acc.: 82.81%] [G loss: 3.337461]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "954 [D loss: 0.317181, acc.: 85.94%] [G loss: 4.673498]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "955 [D loss: 0.119139, acc.: 96.88%] [G loss: 4.383149]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "956 [D loss: 0.156117, acc.: 92.19%] [G loss: 3.211430]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "957 [D loss: 0.342587, acc.: 82.81%] [G loss: 4.271683]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "958 [D loss: 0.518164, acc.: 70.31%] [G loss: 3.572475]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "959 [D loss: 0.398229, acc.: 79.69%] [G loss: 3.082376]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "960 [D loss: 0.072479, acc.: 100.00%] [G loss: 4.053329]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "961 [D loss: 0.251910, acc.: 90.62%] [G loss: 2.975539]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "962 [D loss: 0.197863, acc.: 93.75%] [G loss: 4.347432]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "963 [D loss: 0.333770, acc.: 90.62%] [G loss: 2.916545]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "964 [D loss: 0.126215, acc.: 98.44%] [G loss: 3.875975]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "965 [D loss: 0.203300, acc.: 93.75%] [G loss: 3.569545]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "966 [D loss: 0.479776, acc.: 75.00%] [G loss: 3.523937]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "967 [D loss: 0.260001, acc.: 89.06%] [G loss: 3.648537]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "968 [D loss: 0.348684, acc.: 87.50%] [G loss: 3.543235]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "969 [D loss: 0.305281, acc.: 89.06%] [G loss: 4.454212]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "970 [D loss: 0.579150, acc.: 68.75%] [G loss: 2.948637]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "971 [D loss: 0.165680, acc.: 92.19%] [G loss: 4.520233]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "972 [D loss: 0.104539, acc.: 98.44%] [G loss: 4.004868]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "973 [D loss: 0.160196, acc.: 95.31%] [G loss: 4.791556]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "974 [D loss: 0.208578, acc.: 90.62%] [G loss: 4.419714]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "975 [D loss: 0.132707, acc.: 93.75%] [G loss: 3.570804]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "976 [D loss: 0.433031, acc.: 76.56%] [G loss: 2.557276]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "977 [D loss: 0.247784, acc.: 90.62%] [G loss: 4.930302]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "978 [D loss: 0.637170, acc.: 65.62%] [G loss: 2.876852]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "979 [D loss: 0.231495, acc.: 90.62%] [G loss: 4.473503]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "980 [D loss: 0.057982, acc.: 100.00%] [G loss: 5.466694]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "981 [D loss: 0.925967, acc.: 50.00%] [G loss: 3.316861]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "982 [D loss: 0.213324, acc.: 95.31%] [G loss: 3.708811]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "983 [D loss: 0.359450, acc.: 85.94%] [G loss: 3.131327]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "984 [D loss: 0.248163, acc.: 85.94%] [G loss: 3.715694]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "985 [D loss: 0.171294, acc.: 93.75%] [G loss: 3.470944]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "986 [D loss: 0.149986, acc.: 95.31%] [G loss: 4.917871]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "987 [D loss: 0.264135, acc.: 89.06%] [G loss: 3.540570]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "988 [D loss: 0.450049, acc.: 78.12%] [G loss: 4.540415]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "989 [D loss: 0.197518, acc.: 92.19%] [G loss: 3.830093]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "990 [D loss: 0.243435, acc.: 93.75%] [G loss: 3.485812]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "991 [D loss: 0.079278, acc.: 100.00%] [G loss: 4.093065]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "992 [D loss: 1.059056, acc.: 43.75%] [G loss: 4.390437]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "993 [D loss: 0.161353, acc.: 96.88%] [G loss: 5.996276]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "994 [D loss: 0.537948, acc.: 79.69%] [G loss: 3.142627]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "995 [D loss: 0.148270, acc.: 96.88%] [G loss: 3.857398]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "996 [D loss: 0.255334, acc.: 93.75%] [G loss: 3.540814]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "997 [D loss: 0.345708, acc.: 89.06%] [G loss: 4.261611]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "998 [D loss: 0.357109, acc.: 82.81%] [G loss: 3.387843]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "999 [D loss: 0.386477, acc.: 81.25%] [G loss: 4.168171]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1000 [D loss: 0.169361, acc.: 95.31%] [G loss: 2.833362]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_1000.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1001 [D loss: 0.101647, acc.: 96.88%] [G loss: 4.392871]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1002 [D loss: 0.181586, acc.: 95.31%] [G loss: 4.026919]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1003 [D loss: 0.603782, acc.: 64.06%] [G loss: 3.834274]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1004 [D loss: 0.567508, acc.: 70.31%] [G loss: 3.626247]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1005 [D loss: 0.217233, acc.: 89.06%] [G loss: 2.384495]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1006 [D loss: 0.264386, acc.: 87.50%] [G loss: 2.998344]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1007 [D loss: 0.120678, acc.: 98.44%] [G loss: 2.642009]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1008 [D loss: 0.099387, acc.: 98.44%] [G loss: 4.678007]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1009 [D loss: 0.213145, acc.: 92.19%] [G loss: 3.526325]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1010 [D loss: 0.227631, acc.: 90.62%] [G loss: 3.163513]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1011 [D loss: 0.123235, acc.: 96.88%] [G loss: 4.469350]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1012 [D loss: 0.169080, acc.: 96.88%] [G loss: 3.568504]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1013 [D loss: 0.755274, acc.: 54.69%] [G loss: 4.800260]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1014 [D loss: 0.449718, acc.: 78.12%] [G loss: 4.148928]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1015 [D loss: 0.317441, acc.: 89.06%] [G loss: 5.138796]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1016 [D loss: 0.424442, acc.: 78.12%] [G loss: 3.006992]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1017 [D loss: 0.344972, acc.: 82.81%] [G loss: 5.503431]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1018 [D loss: 0.840424, acc.: 54.69%] [G loss: 2.126822]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1019 [D loss: 0.204579, acc.: 92.19%] [G loss: 3.822334]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1020 [D loss: 0.812977, acc.: 57.81%] [G loss: 3.100241]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1021 [D loss: 0.470336, acc.: 81.25%] [G loss: 4.231243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1022 [D loss: 0.163898, acc.: 92.19%] [G loss: 4.725245]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1023 [D loss: 0.136850, acc.: 95.31%] [G loss: 4.105426]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1024 [D loss: 0.370553, acc.: 78.12%] [G loss: 2.790691]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1025 [D loss: 0.224097, acc.: 93.75%] [G loss: 3.872099]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1026 [D loss: 0.635531, acc.: 64.06%] [G loss: 3.374310]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1027 [D loss: 0.268268, acc.: 87.50%] [G loss: 4.545551]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1028 [D loss: 0.218535, acc.: 93.75%] [G loss: 4.445969]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1029 [D loss: 0.391708, acc.: 78.12%] [G loss: 2.679930]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1030 [D loss: 0.151878, acc.: 95.31%] [G loss: 4.281457]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1031 [D loss: 0.202103, acc.: 93.75%] [G loss: 3.227947]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1032 [D loss: 0.141666, acc.: 95.31%] [G loss: 4.527422]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1033 [D loss: 0.460996, acc.: 76.56%] [G loss: 2.733927]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1034 [D loss: 0.137618, acc.: 96.88%] [G loss: 4.139620]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1035 [D loss: 0.168818, acc.: 95.31%] [G loss: 4.255944]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1036 [D loss: 0.544964, acc.: 70.31%] [G loss: 4.698570]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1037 [D loss: 0.811042, acc.: 54.69%] [G loss: 3.112957]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1038 [D loss: 0.445682, acc.: 78.12%] [G loss: 3.518689]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1039 [D loss: 0.393187, acc.: 81.25%] [G loss: 4.583861]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1040 [D loss: 0.268290, acc.: 92.19%] [G loss: 4.319469]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1041 [D loss: 0.867321, acc.: 50.00%] [G loss: 3.598503]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1042 [D loss: 0.136498, acc.: 96.88%] [G loss: 4.513743]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1043 [D loss: 0.633773, acc.: 70.31%] [G loss: 3.182816]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1044 [D loss: 0.096135, acc.: 100.00%] [G loss: 3.612892]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1045 [D loss: 0.164287, acc.: 96.88%] [G loss: 4.697259]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1046 [D loss: 0.255933, acc.: 90.62%] [G loss: 5.631799]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1047 [D loss: 0.297311, acc.: 85.94%] [G loss: 4.440722]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1048 [D loss: 0.348293, acc.: 82.81%] [G loss: 3.470461]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1049 [D loss: 0.214445, acc.: 93.75%] [G loss: 4.227782]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1050 [D loss: 0.274498, acc.: 87.50%] [G loss: 2.688931]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_1050.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1051 [D loss: 0.109760, acc.: 98.44%] [G loss: 4.202443]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1052 [D loss: 0.337173, acc.: 81.25%] [G loss: 4.395060]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1053 [D loss: 0.337680, acc.: 85.94%] [G loss: 3.894194]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1054 [D loss: 0.404811, acc.: 75.00%] [G loss: 5.067544]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1055 [D loss: 0.413615, acc.: 82.81%] [G loss: 4.498055]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1056 [D loss: 0.560708, acc.: 71.88%] [G loss: 3.981396]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1057 [D loss: 0.326094, acc.: 90.62%] [G loss: 4.180037]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1058 [D loss: 0.364342, acc.: 84.38%] [G loss: 2.777883]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1059 [D loss: 0.184267, acc.: 96.88%] [G loss: 4.116949]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1060 [D loss: 0.451847, acc.: 73.44%] [G loss: 2.641860]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1061 [D loss: 0.285489, acc.: 82.81%] [G loss: 5.275961]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1062 [D loss: 0.442582, acc.: 81.25%] [G loss: 3.695969]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1063 [D loss: 0.246201, acc.: 93.75%] [G loss: 4.352781]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1064 [D loss: 0.107432, acc.: 96.88%] [G loss: 4.763857]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1065 [D loss: 0.379239, acc.: 85.94%] [G loss: 3.586741]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1066 [D loss: 0.396855, acc.: 73.44%] [G loss: 2.317626]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1067 [D loss: 0.312890, acc.: 85.94%] [G loss: 4.220665]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1068 [D loss: 0.106796, acc.: 98.44%] [G loss: 4.913179]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1069 [D loss: 0.136777, acc.: 98.44%] [G loss: 3.227769]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1070 [D loss: 0.165981, acc.: 95.31%] [G loss: 4.215430]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1071 [D loss: 0.188561, acc.: 93.75%] [G loss: 4.622951]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1072 [D loss: 0.618533, acc.: 65.62%] [G loss: 5.051778]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1073 [D loss: 0.045728, acc.: 98.44%] [G loss: 6.993143]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1074 [D loss: 0.710773, acc.: 70.31%] [G loss: 3.534478]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1075 [D loss: 0.172489, acc.: 90.62%] [G loss: 4.163339]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1076 [D loss: 0.232967, acc.: 93.75%] [G loss: 4.116190]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1077 [D loss: 0.953690, acc.: 50.00%] [G loss: 3.976010]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1078 [D loss: 0.133901, acc.: 95.31%] [G loss: 4.435168]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1079 [D loss: 0.791400, acc.: 62.50%] [G loss: 2.822027]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1080 [D loss: 0.411834, acc.: 76.56%] [G loss: 3.984036]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1081 [D loss: 0.923468, acc.: 43.75%] [G loss: 2.440952]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1082 [D loss: 0.195793, acc.: 92.19%] [G loss: 4.839369]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1083 [D loss: 0.323540, acc.: 84.38%] [G loss: 3.165672]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1084 [D loss: 0.081342, acc.: 100.00%] [G loss: 3.559629]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1085 [D loss: 0.373663, acc.: 79.69%] [G loss: 5.031713]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1086 [D loss: 0.575080, acc.: 70.31%] [G loss: 2.762240]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1087 [D loss: 0.536354, acc.: 76.56%] [G loss: 4.270864]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1088 [D loss: 0.069221, acc.: 100.00%] [G loss: 5.578367]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1089 [D loss: 0.456159, acc.: 79.69%] [G loss: 3.003851]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1090 [D loss: 0.639806, acc.: 70.31%] [G loss: 4.476207]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1091 [D loss: 0.298266, acc.: 87.50%] [G loss: 4.414924]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1092 [D loss: 0.154403, acc.: 96.88%] [G loss: 4.345153]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1093 [D loss: 0.282309, acc.: 89.06%] [G loss: 3.819451]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1094 [D loss: 0.093240, acc.: 100.00%] [G loss: 4.472818]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1095 [D loss: 0.126212, acc.: 96.88%] [G loss: 4.372038]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1096 [D loss: 0.129638, acc.: 95.31%] [G loss: 1.981423]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1097 [D loss: 0.323397, acc.: 84.38%] [G loss: 3.395880]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1098 [D loss: 0.262972, acc.: 90.62%] [G loss: 4.213799]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1099 [D loss: 0.567365, acc.: 70.31%] [G loss: 3.905555]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1100 [D loss: 0.092078, acc.: 96.88%] [G loss: 4.960856]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_1100.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1101 [D loss: 0.367641, acc.: 79.69%] [G loss: 2.837434]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1102 [D loss: 0.167952, acc.: 93.75%] [G loss: 3.495032]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1103 [D loss: 0.165914, acc.: 96.88%] [G loss: 4.884477]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1104 [D loss: 0.754674, acc.: 56.25%] [G loss: 4.081653]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1105 [D loss: 0.382292, acc.: 82.81%] [G loss: 4.844410]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1106 [D loss: 1.006271, acc.: 45.31%] [G loss: 2.649036]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1107 [D loss: 0.723982, acc.: 54.69%] [G loss: 4.687180]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1108 [D loss: 0.554126, acc.: 73.44%] [G loss: 2.709614]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1109 [D loss: 0.417135, acc.: 73.44%] [G loss: 5.403193]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1110 [D loss: 0.548921, acc.: 62.50%] [G loss: 4.431485]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1111 [D loss: 0.177557, acc.: 93.75%] [G loss: 3.785637]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1112 [D loss: 0.090028, acc.: 100.00%] [G loss: 4.198984]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1113 [D loss: 0.285699, acc.: 89.06%] [G loss: 5.030080]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1114 [D loss: 0.134876, acc.: 96.88%] [G loss: 4.392762]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1115 [D loss: 0.738150, acc.: 53.12%] [G loss: 3.980129]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1116 [D loss: 0.114150, acc.: 98.44%] [G loss: 5.142360]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1117 [D loss: 0.025690, acc.: 100.00%] [G loss: 5.004958]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118 [D loss: 0.555285, acc.: 71.88%] [G loss: 2.187365]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1119 [D loss: 0.704497, acc.: 64.06%] [G loss: 5.172221]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1120 [D loss: 0.295744, acc.: 85.94%] [G loss: 4.410106]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1121 [D loss: 0.210816, acc.: 93.75%] [G loss: 3.314707]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1122 [D loss: 0.171078, acc.: 93.75%] [G loss: 3.725232]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1123 [D loss: 0.402273, acc.: 76.56%] [G loss: 2.833071]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1124 [D loss: 0.287821, acc.: 85.94%] [G loss: 4.486867]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1125 [D loss: 0.415916, acc.: 75.00%] [G loss: 2.576658]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1126 [D loss: 0.142375, acc.: 93.75%] [G loss: 4.366669]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1127 [D loss: 0.213916, acc.: 93.75%] [G loss: 3.591139]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1128 [D loss: 0.402018, acc.: 78.12%] [G loss: 4.416054]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1129 [D loss: 0.158402, acc.: 95.31%] [G loss: 4.925983]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1130 [D loss: 0.310304, acc.: 84.38%] [G loss: 2.947264]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1131 [D loss: 0.087021, acc.: 98.44%] [G loss: 3.717215]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1132 [D loss: 0.275263, acc.: 89.06%] [G loss: 4.673122]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1133 [D loss: 0.425716, acc.: 79.69%] [G loss: 2.329287]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1134 [D loss: 0.134837, acc.: 93.75%] [G loss: 3.611802]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1135 [D loss: 0.141643, acc.: 100.00%] [G loss: 5.119551]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1136 [D loss: 0.235310, acc.: 95.31%] [G loss: 4.217317]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1137 [D loss: 0.666537, acc.: 68.75%] [G loss: 3.263949]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1138 [D loss: 0.339896, acc.: 87.50%] [G loss: 5.249238]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1139 [D loss: 0.113187, acc.: 95.31%] [G loss: 6.949790]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1140 [D loss: 0.990749, acc.: 57.81%] [G loss: 1.559211]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1141 [D loss: 0.733168, acc.: 68.75%] [G loss: 5.790697]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1142 [D loss: 0.202661, acc.: 93.75%] [G loss: 5.720937]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1143 [D loss: 0.330627, acc.: 89.06%] [G loss: 3.284685]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1144 [D loss: 0.292804, acc.: 92.19%] [G loss: 3.118306]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1145 [D loss: 0.262783, acc.: 89.06%] [G loss: 4.057456]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1146 [D loss: 0.863758, acc.: 48.44%] [G loss: 3.765248]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1147 [D loss: 0.288647, acc.: 92.19%] [G loss: 3.885230]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1148 [D loss: 0.676140, acc.: 57.81%] [G loss: 4.737332]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1149 [D loss: 0.134701, acc.: 96.88%] [G loss: 5.478367]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1150 [D loss: 0.537044, acc.: 67.19%] [G loss: 4.323869]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_1150.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1151 [D loss: 0.186075, acc.: 95.31%] [G loss: 3.973625]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1152 [D loss: 0.251207, acc.: 87.50%] [G loss: 3.722975]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1153 [D loss: 0.223305, acc.: 92.19%] [G loss: 3.456909]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1154 [D loss: 0.841924, acc.: 48.44%] [G loss: 3.519796]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1155 [D loss: 0.205629, acc.: 93.75%] [G loss: 4.597116]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1156 [D loss: 0.430444, acc.: 79.69%] [G loss: 3.145979]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1157 [D loss: 0.224779, acc.: 90.62%] [G loss: 3.535041]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1158 [D loss: 0.136774, acc.: 98.44%] [G loss: 4.885746]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1159 [D loss: 0.335296, acc.: 82.81%] [G loss: 2.840881]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1160 [D loss: 0.203397, acc.: 93.75%] [G loss: 3.848425]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1161 [D loss: 0.191299, acc.: 92.19%] [G loss: 5.349409]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1162 [D loss: 0.312809, acc.: 84.38%] [G loss: 2.782347]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1163 [D loss: 0.205166, acc.: 93.75%] [G loss: 4.469171]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1164 [D loss: 0.300520, acc.: 89.06%] [G loss: 5.040218]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1165 [D loss: 0.199379, acc.: 92.19%] [G loss: 4.155589]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1166 [D loss: 0.339449, acc.: 87.50%] [G loss: 4.388787]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1167 [D loss: 0.295612, acc.: 87.50%] [G loss: 4.258986]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1168 [D loss: 0.309945, acc.: 87.50%] [G loss: 3.426373]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1169 [D loss: 0.337070, acc.: 84.38%] [G loss: 4.140248]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1170 [D loss: 0.280121, acc.: 89.06%] [G loss: 4.777530]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1171 [D loss: 0.616472, acc.: 68.75%] [G loss: 4.177958]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1172 [D loss: 0.133611, acc.: 96.88%] [G loss: 4.087684]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1173 [D loss: 0.330083, acc.: 84.38%] [G loss: 3.352624]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1174 [D loss: 0.140096, acc.: 96.88%] [G loss: 4.377242]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1175 [D loss: 0.202910, acc.: 96.88%] [G loss: 3.432063]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1176 [D loss: 1.070658, acc.: 45.31%] [G loss: 3.320680]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1177 [D loss: 0.271609, acc.: 90.62%] [G loss: 3.472709]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1178 [D loss: 0.144889, acc.: 93.75%] [G loss: 4.020818]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1179 [D loss: 0.409468, acc.: 81.25%] [G loss: 4.398129]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1180 [D loss: 0.235228, acc.: 93.75%] [G loss: 3.422857]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1181 [D loss: 0.176783, acc.: 95.31%] [G loss: 3.666883]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1182 [D loss: 0.098589, acc.: 98.44%] [G loss: 3.225851]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1183 [D loss: 0.281452, acc.: 87.50%] [G loss: 2.446656]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1184 [D loss: 0.174892, acc.: 93.75%] [G loss: 3.121561]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1185 [D loss: 0.256772, acc.: 90.62%] [G loss: 4.154269]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1186 [D loss: 0.352947, acc.: 87.50%] [G loss: 2.734500]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1187 [D loss: 0.202074, acc.: 90.62%] [G loss: 3.380726]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1188 [D loss: 0.081939, acc.: 98.44%] [G loss: 3.524141]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1189 [D loss: 0.160101, acc.: 95.31%] [G loss: 4.416427]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1190 [D loss: 0.153313, acc.: 96.88%] [G loss: 4.120070]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1191 [D loss: 0.305682, acc.: 85.94%] [G loss: 4.192951]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1192 [D loss: 0.083189, acc.: 100.00%] [G loss: 5.706586]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1193 [D loss: 0.069484, acc.: 100.00%] [G loss: 4.523361]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1194 [D loss: 0.207294, acc.: 93.75%] [G loss: 4.015175]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1195 [D loss: 0.246993, acc.: 90.62%] [G loss: 3.309223]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1196 [D loss: 0.187583, acc.: 92.19%] [G loss: 3.427003]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1197 [D loss: 0.202187, acc.: 93.75%] [G loss: 4.427102]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1198 [D loss: 0.367476, acc.: 84.38%] [G loss: 3.125056]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1199 [D loss: 0.487721, acc.: 73.44%] [G loss: 3.274168]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1200 [D loss: 0.145293, acc.: 95.31%] [G loss: 5.572742]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_1200.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1201 [D loss: 0.681000, acc.: 65.62%] [G loss: 2.541373]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1202 [D loss: 0.210015, acc.: 89.06%] [G loss: 4.534356]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1203 [D loss: 0.112321, acc.: 95.31%] [G loss: 5.695107]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1204 [D loss: 0.350867, acc.: 85.94%] [G loss: 3.076825]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1205 [D loss: 0.180296, acc.: 96.88%] [G loss: 4.035085]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1206 [D loss: 0.102591, acc.: 96.88%] [G loss: 3.581668]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1207 [D loss: 0.826138, acc.: 57.81%] [G loss: 3.938939]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1208 [D loss: 0.561019, acc.: 70.31%] [G loss: 2.404762]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1209 [D loss: 0.287427, acc.: 87.50%] [G loss: 4.820564]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1210 [D loss: 0.457655, acc.: 75.00%] [G loss: 4.098479]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1211 [D loss: 0.484129, acc.: 67.19%] [G loss: 3.160718]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1212 [D loss: 0.611686, acc.: 65.62%] [G loss: 4.643129]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1213 [D loss: 0.162472, acc.: 92.19%] [G loss: 4.562746]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1214 [D loss: 0.346916, acc.: 84.38%] [G loss: 4.758663]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1215 [D loss: 0.342912, acc.: 82.81%] [G loss: 3.292866]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1216 [D loss: 0.381139, acc.: 82.81%] [G loss: 6.015707]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1217 [D loss: 1.246364, acc.: 43.75%] [G loss: 2.199732]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1218 [D loss: 0.449977, acc.: 76.56%] [G loss: 5.718280]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1219 [D loss: 0.318913, acc.: 76.56%] [G loss: 3.415782]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1220 [D loss: 0.240879, acc.: 89.06%] [G loss: 3.927891]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1221 [D loss: 0.382548, acc.: 81.25%] [G loss: 4.452518]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1222 [D loss: 0.130782, acc.: 95.31%] [G loss: 4.464730]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1223 [D loss: 0.298465, acc.: 87.50%] [G loss: 4.196176]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1224 [D loss: 0.382048, acc.: 84.38%] [G loss: 3.175179]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1225 [D loss: 0.075428, acc.: 98.44%] [G loss: 5.084447]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1226 [D loss: 0.185669, acc.: 95.31%] [G loss: 2.578013]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1227 [D loss: 0.104270, acc.: 96.88%] [G loss: 3.355169]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1228 [D loss: 0.098229, acc.: 96.88%] [G loss: 4.518828]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1229 [D loss: 0.131586, acc.: 98.44%] [G loss: 4.238299]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1230 [D loss: 0.490428, acc.: 78.12%] [G loss: 2.512278]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1231 [D loss: 0.112106, acc.: 100.00%] [G loss: 4.127272]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1232 [D loss: 0.254930, acc.: 85.94%] [G loss: 4.262777]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1233 [D loss: 0.170340, acc.: 98.44%] [G loss: 4.148168]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1234 [D loss: 0.397359, acc.: 81.25%] [G loss: 3.736153]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1235 [D loss: 0.113408, acc.: 96.88%] [G loss: 5.252751]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1236 [D loss: 0.357343, acc.: 82.81%] [G loss: 2.971385]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1237 [D loss: 0.240072, acc.: 85.94%] [G loss: 5.387566]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1238 [D loss: 0.270065, acc.: 89.06%] [G loss: 4.063962]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1239 [D loss: 0.196415, acc.: 92.19%] [G loss: 3.957914]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1240 [D loss: 0.129726, acc.: 98.44%] [G loss: 3.966978]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1241 [D loss: 0.243392, acc.: 92.19%] [G loss: 3.676574]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1242 [D loss: 0.333941, acc.: 78.12%] [G loss: 3.001698]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1243 [D loss: 0.307028, acc.: 84.38%] [G loss: 3.721838]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1244 [D loss: 0.461053, acc.: 81.25%] [G loss: 3.504646]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1245 [D loss: 0.304915, acc.: 84.38%] [G loss: 5.405893]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1246 [D loss: 0.232238, acc.: 87.50%] [G loss: 4.707006]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1247 [D loss: 0.179120, acc.: 93.75%] [G loss: 4.663020]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1248 [D loss: 0.292993, acc.: 87.50%] [G loss: 4.101141]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1249 [D loss: 0.439015, acc.: 81.25%] [G loss: 3.718640]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1250 [D loss: 0.327346, acc.: 84.38%] [G loss: 4.717896]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_1250.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1251 [D loss: 0.147139, acc.: 96.88%] [G loss: 4.938773]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1252 [D loss: 0.114104, acc.: 96.88%] [G loss: 3.910092]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1253 [D loss: 0.264088, acc.: 89.06%] [G loss: 4.568532]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1254 [D loss: 0.225773, acc.: 93.75%] [G loss: 4.320829]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1255 [D loss: 0.827600, acc.: 53.12%] [G loss: 3.723461]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1256 [D loss: 0.162474, acc.: 96.88%] [G loss: 3.732850]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1257 [D loss: 0.377684, acc.: 82.81%] [G loss: 2.877982]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1258 [D loss: 0.159103, acc.: 98.44%] [G loss: 4.467565]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1259 [D loss: 0.185337, acc.: 95.31%] [G loss: 3.839884]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1260 [D loss: 0.185880, acc.: 93.75%] [G loss: 3.770865]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1261 [D loss: 0.113270, acc.: 98.44%] [G loss: 3.756883]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1262 [D loss: 0.102619, acc.: 100.00%] [G loss: 3.986846]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1263 [D loss: 0.195220, acc.: 92.19%] [G loss: 3.617578]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1264 [D loss: 0.141080, acc.: 93.75%] [G loss: 5.124682]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1265 [D loss: 0.242845, acc.: 90.62%] [G loss: 3.836517]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1266 [D loss: 0.468785, acc.: 76.56%] [G loss: 3.502095]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1267 [D loss: 0.126895, acc.: 100.00%] [G loss: 3.701078]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1268 [D loss: 0.682283, acc.: 59.38%] [G loss: 3.337001]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1269 [D loss: 0.088584, acc.: 100.00%] [G loss: 4.887175]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1270 [D loss: 0.245665, acc.: 93.75%] [G loss: 4.131415]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1271 [D loss: 0.724054, acc.: 56.25%] [G loss: 4.842218]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1272 [D loss: 0.083298, acc.: 98.44%] [G loss: 4.643859]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1273 [D loss: 0.933920, acc.: 51.56%] [G loss: 3.164429]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1274 [D loss: 0.134082, acc.: 95.31%] [G loss: 4.792955]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1275 [D loss: 0.355862, acc.: 85.94%] [G loss: 3.405118]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1276 [D loss: 0.247613, acc.: 90.62%] [G loss: 3.351025]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1277 [D loss: 0.285301, acc.: 92.19%] [G loss: 4.344355]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1278 [D loss: 0.246082, acc.: 90.62%] [G loss: 3.575916]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1279 [D loss: 0.264583, acc.: 87.50%] [G loss: 3.460883]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1280 [D loss: 0.471528, acc.: 76.56%] [G loss: 4.949606]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1281 [D loss: 0.332309, acc.: 79.69%] [G loss: 3.411927]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1282 [D loss: 0.112024, acc.: 96.88%] [G loss: 4.470238]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1283 [D loss: 0.218704, acc.: 92.19%] [G loss: 4.339022]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1284 [D loss: 0.408804, acc.: 75.00%] [G loss: 4.551949]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1285 [D loss: 0.427043, acc.: 76.56%] [G loss: 3.549388]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1286 [D loss: 0.650191, acc.: 62.50%] [G loss: 3.961883]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1287 [D loss: 0.493833, acc.: 73.44%] [G loss: 3.982689]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1288 [D loss: 0.176674, acc.: 93.75%] [G loss: 4.499666]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1289 [D loss: 0.301473, acc.: 85.94%] [G loss: 4.771472]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1290 [D loss: 0.082045, acc.: 98.44%] [G loss: 4.783514]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1291 [D loss: 0.147830, acc.: 98.44%] [G loss: 2.876600]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1292 [D loss: 0.145233, acc.: 96.88%] [G loss: 4.181973]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1293 [D loss: 0.281954, acc.: 89.06%] [G loss: 4.052360]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1294 [D loss: 0.159457, acc.: 95.31%] [G loss: 3.579009]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1295 [D loss: 0.254041, acc.: 90.62%] [G loss: 2.768972]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1296 [D loss: 0.287802, acc.: 87.50%] [G loss: 5.834217]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1297 [D loss: 0.228381, acc.: 90.62%] [G loss: 3.781374]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1298 [D loss: 0.406440, acc.: 81.25%] [G loss: 5.330208]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1299 [D loss: 0.279840, acc.: 90.62%] [G loss: 5.074657]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1300 [D loss: 0.793822, acc.: 56.25%] [G loss: 4.419978]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_1300.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1301 [D loss: 0.029786, acc.: 100.00%] [G loss: 7.249057]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1302 [D loss: 2.563840, acc.: 10.94%] [G loss: 6.236182]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1303 [D loss: 1.680509, acc.: 39.06%] [G loss: 3.282285]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1304 [D loss: 0.138948, acc.: 95.31%] [G loss: 5.150307]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1305 [D loss: 1.664494, acc.: 26.56%] [G loss: 4.087007]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1306 [D loss: 0.317570, acc.: 84.38%] [G loss: 5.923197]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1307 [D loss: 0.336858, acc.: 85.94%] [G loss: 3.176563]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1308 [D loss: 0.355172, acc.: 79.69%] [G loss: 3.737385]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1309 [D loss: 0.110377, acc.: 98.44%] [G loss: 4.508042]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1310 [D loss: 0.170566, acc.: 93.75%] [G loss: 3.959910]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1311 [D loss: 0.207163, acc.: 89.06%] [G loss: 4.230322]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1312 [D loss: 0.392024, acc.: 85.94%] [G loss: 2.858485]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1313 [D loss: 0.296113, acc.: 85.94%] [G loss: 3.126498]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1314 [D loss: 0.128448, acc.: 98.44%] [G loss: 2.918360]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1315 [D loss: 0.638995, acc.: 64.06%] [G loss: 4.161168]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1316 [D loss: 0.065359, acc.: 98.44%] [G loss: 6.353001]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1317 [D loss: 0.564826, acc.: 65.62%] [G loss: 2.620178]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1318 [D loss: 0.091146, acc.: 98.44%] [G loss: 4.021770]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1319 [D loss: 0.407183, acc.: 78.12%] [G loss: 3.908796]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1320 [D loss: 0.745599, acc.: 57.81%] [G loss: 3.419595]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1321 [D loss: 0.224236, acc.: 90.62%] [G loss: 4.934987]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1322 [D loss: 1.490961, acc.: 35.94%] [G loss: 3.331308]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1323 [D loss: 0.173612, acc.: 92.19%] [G loss: 4.553810]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1324 [D loss: 0.202634, acc.: 93.75%] [G loss: 4.123753]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1325 [D loss: 0.118253, acc.: 95.31%] [G loss: 4.521473]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1326 [D loss: 0.226719, acc.: 89.06%] [G loss: 4.006409]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1327 [D loss: 0.608682, acc.: 65.62%] [G loss: 2.976532]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1328 [D loss: 0.265087, acc.: 89.06%] [G loss: 3.518416]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1329 [D loss: 0.415919, acc.: 76.56%] [G loss: 3.542837]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1330 [D loss: 0.133695, acc.: 95.31%] [G loss: 5.081785]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1331 [D loss: 0.572400, acc.: 75.00%] [G loss: 4.740643]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1332 [D loss: 0.286863, acc.: 87.50%] [G loss: 4.004661]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1333 [D loss: 0.265654, acc.: 90.62%] [G loss: 4.239041]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1334 [D loss: 0.229542, acc.: 87.50%] [G loss: 3.947694]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1335 [D loss: 0.144740, acc.: 96.88%] [G loss: 2.946841]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1336 [D loss: 0.198210, acc.: 93.75%] [G loss: 4.105098]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1337 [D loss: 0.349160, acc.: 82.81%] [G loss: 4.005019]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1338 [D loss: 0.129072, acc.: 95.31%] [G loss: 5.103553]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1339 [D loss: 0.472798, acc.: 76.56%] [G loss: 3.354786]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1340 [D loss: 0.125764, acc.: 98.44%] [G loss: 4.733440]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1341 [D loss: 0.101926, acc.: 98.44%] [G loss: 4.567379]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1342 [D loss: 0.265343, acc.: 92.19%] [G loss: 3.091347]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1343 [D loss: 0.182168, acc.: 93.75%] [G loss: 3.771503]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1344 [D loss: 0.080146, acc.: 100.00%] [G loss: 3.857605]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1345 [D loss: 0.118032, acc.: 98.44%] [G loss: 3.821976]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1346 [D loss: 0.259923, acc.: 93.75%] [G loss: 3.345075]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1347 [D loss: 0.401257, acc.: 78.12%] [G loss: 2.615960]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1348 [D loss: 0.831905, acc.: 64.06%] [G loss: 6.322897]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1349 [D loss: 0.510973, acc.: 71.88%] [G loss: 2.591677]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1350 [D loss: 0.386400, acc.: 82.81%] [G loss: 3.505911]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_1350.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1351 [D loss: 0.131058, acc.: 96.88%] [G loss: 3.602417]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1352 [D loss: 0.150506, acc.: 95.31%] [G loss: 4.658805]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1353 [D loss: 1.348711, acc.: 31.25%] [G loss: 3.654315]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1354 [D loss: 0.033142, acc.: 100.00%] [G loss: 6.573917]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1355 [D loss: 1.194754, acc.: 56.25%] [G loss: 3.747999]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1356 [D loss: 0.270448, acc.: 92.19%] [G loss: 3.445699]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1357 [D loss: 0.609225, acc.: 64.06%] [G loss: 5.027013]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1358 [D loss: 0.151661, acc.: 96.88%] [G loss: 4.143149]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1359 [D loss: 0.143318, acc.: 93.75%] [G loss: 3.944820]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1360 [D loss: 0.180517, acc.: 95.31%] [G loss: 3.116652]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1361 [D loss: 0.160955, acc.: 95.31%] [G loss: 3.310080]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1362 [D loss: 0.115363, acc.: 96.88%] [G loss: 5.089565]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1363 [D loss: 0.474444, acc.: 73.44%] [G loss: 3.586430]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1364 [D loss: 0.133178, acc.: 98.44%] [G loss: 4.134254]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1365 [D loss: 0.200136, acc.: 93.75%] [G loss: 3.775565]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1366 [D loss: 0.499399, acc.: 75.00%] [G loss: 3.223609]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1367 [D loss: 0.071580, acc.: 98.44%] [G loss: 4.514749]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1368 [D loss: 0.180453, acc.: 96.88%] [G loss: 2.992003]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1369 [D loss: 0.260293, acc.: 90.62%] [G loss: 3.755697]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1370 [D loss: 0.325396, acc.: 89.06%] [G loss: 4.068247]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1371 [D loss: 0.213091, acc.: 93.75%] [G loss: 3.108634]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1372 [D loss: 0.473893, acc.: 71.88%] [G loss: 3.089742]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1373 [D loss: 0.161916, acc.: 96.88%] [G loss: 3.838630]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1374 [D loss: 0.045510, acc.: 100.00%] [G loss: 4.774515]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1375 [D loss: 0.075074, acc.: 100.00%] [G loss: 4.277018]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1376 [D loss: 0.276515, acc.: 93.75%] [G loss: 4.388400]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1377 [D loss: 0.188818, acc.: 92.19%] [G loss: 4.085145]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1378 [D loss: 0.427856, acc.: 84.38%] [G loss: 2.998040]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1379 [D loss: 0.114903, acc.: 96.88%] [G loss: 3.609925]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1380 [D loss: 0.083045, acc.: 100.00%] [G loss: 4.512835]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1381 [D loss: 0.110713, acc.: 98.44%] [G loss: 4.145899]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1382 [D loss: 0.307877, acc.: 87.50%] [G loss: 2.850190]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1383 [D loss: 0.327501, acc.: 82.81%] [G loss: 4.465446]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1384 [D loss: 0.131989, acc.: 98.44%] [G loss: 3.898188]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1385 [D loss: 0.154832, acc.: 95.31%] [G loss: 4.418775]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1386 [D loss: 0.271766, acc.: 92.19%] [G loss: 3.394243]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1387 [D loss: 0.111890, acc.: 100.00%] [G loss: 3.908217]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1388 [D loss: 0.112079, acc.: 100.00%] [G loss: 4.349429]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1389 [D loss: 1.055732, acc.: 37.50%] [G loss: 3.594342]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1390 [D loss: 1.095126, acc.: 39.06%] [G loss: 2.987652]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1391 [D loss: 0.142395, acc.: 95.31%] [G loss: 4.638714]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1392 [D loss: 0.560421, acc.: 75.00%] [G loss: 4.196754]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1393 [D loss: 0.340732, acc.: 90.62%] [G loss: 3.017156]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1394 [D loss: 0.177255, acc.: 92.19%] [G loss: 2.963119]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1395 [D loss: 0.295621, acc.: 89.06%] [G loss: 4.405804]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1396 [D loss: 0.080990, acc.: 98.44%] [G loss: 3.770715]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1397 [D loss: 0.356699, acc.: 81.25%] [G loss: 3.121693]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1398 [D loss: 0.261592, acc.: 89.06%] [G loss: 4.187859]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1399 [D loss: 0.146909, acc.: 95.31%] [G loss: 4.583385]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1400 [D loss: 0.193919, acc.: 95.31%] [G loss: 4.189837]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_1400.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1401 [D loss: 0.222541, acc.: 90.62%] [G loss: 2.423289]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1402 [D loss: 0.209780, acc.: 95.31%] [G loss: 3.344253]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1403 [D loss: 0.185158, acc.: 96.88%] [G loss: 3.903799]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1404 [D loss: 0.078156, acc.: 100.00%] [G loss: 4.792573]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1405 [D loss: 0.243939, acc.: 90.62%] [G loss: 4.149954]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1406 [D loss: 0.307489, acc.: 89.06%] [G loss: 3.729134]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1407 [D loss: 0.224887, acc.: 87.50%] [G loss: 5.008335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1408 [D loss: 0.660387, acc.: 65.62%] [G loss: 2.792977]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1409 [D loss: 0.194870, acc.: 89.06%] [G loss: 4.263715]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1410 [D loss: 0.206828, acc.: 93.75%] [G loss: 5.061658]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1411 [D loss: 0.224954, acc.: 92.19%] [G loss: 2.548120]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1412 [D loss: 0.327813, acc.: 84.38%] [G loss: 3.767294]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1413 [D loss: 0.129504, acc.: 96.88%] [G loss: 3.143580]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1414 [D loss: 0.409410, acc.: 82.81%] [G loss: 3.974868]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1415 [D loss: 0.150447, acc.: 98.44%] [G loss: 4.484338]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1416 [D loss: 0.280818, acc.: 84.38%] [G loss: 2.596449]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1417 [D loss: 0.125057, acc.: 98.44%] [G loss: 3.267471]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1418 [D loss: 0.806371, acc.: 57.81%] [G loss: 3.761582]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1419 [D loss: 0.589759, acc.: 65.62%] [G loss: 4.426099]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1420 [D loss: 0.055896, acc.: 98.44%] [G loss: 6.572145]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1421 [D loss: 0.446236, acc.: 79.69%] [G loss: 2.179847]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1422 [D loss: 0.170904, acc.: 90.62%] [G loss: 3.970287]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1423 [D loss: 0.142052, acc.: 95.31%] [G loss: 5.110924]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1424 [D loss: 0.121291, acc.: 96.88%] [G loss: 3.870819]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1425 [D loss: 0.234145, acc.: 89.06%] [G loss: 3.951133]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1426 [D loss: 0.084422, acc.: 96.88%] [G loss: 4.402949]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1427 [D loss: 0.614176, acc.: 67.19%] [G loss: 3.560328]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1428 [D loss: 0.205613, acc.: 95.31%] [G loss: 4.375859]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1429 [D loss: 0.121317, acc.: 100.00%] [G loss: 3.641244]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1430 [D loss: 0.189799, acc.: 95.31%] [G loss: 2.757021]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1431 [D loss: 0.176213, acc.: 95.31%] [G loss: 4.200308]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1432 [D loss: 0.362119, acc.: 84.38%] [G loss: 3.854186]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1433 [D loss: 0.152714, acc.: 95.31%] [G loss: 3.551884]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1434 [D loss: 0.086130, acc.: 100.00%] [G loss: 4.059167]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1435 [D loss: 0.108155, acc.: 98.44%] [G loss: 4.472066]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1436 [D loss: 0.163755, acc.: 92.19%] [G loss: 4.252900]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1437 [D loss: 0.138894, acc.: 96.88%] [G loss: 4.363266]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1438 [D loss: 0.111987, acc.: 98.44%] [G loss: 4.173067]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1439 [D loss: 0.325910, acc.: 89.06%] [G loss: 3.947408]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1440 [D loss: 0.085946, acc.: 98.44%] [G loss: 2.783443]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1441 [D loss: 0.613475, acc.: 64.06%] [G loss: 3.712096]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1442 [D loss: 1.468908, acc.: 42.19%] [G loss: 5.455723]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1443 [D loss: 0.174633, acc.: 93.75%] [G loss: 5.672143]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1444 [D loss: 0.408022, acc.: 85.94%] [G loss: 3.135380]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1445 [D loss: 0.343330, acc.: 81.25%] [G loss: 5.463068]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1446 [D loss: 0.243126, acc.: 95.31%] [G loss: 5.521037]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1447 [D loss: 0.313450, acc.: 84.38%] [G loss: 4.222591]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1448 [D loss: 0.143462, acc.: 96.88%] [G loss: 4.645687]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1449 [D loss: 0.061937, acc.: 98.44%] [G loss: 3.742563]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1450 [D loss: 0.186214, acc.: 93.75%] [G loss: 3.571231]\n",
      "Save begin!\n",
      "(25, 100, 128, 1)\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_1450.png\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1451 [D loss: 0.440748, acc.: 75.00%] [G loss: 3.250688]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1452 [D loss: 0.154212, acc.: 95.31%] [G loss: 4.282437]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1453 [D loss: 0.488801, acc.: 75.00%] [G loss: 4.562097]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1454 [D loss: 0.208857, acc.: 95.31%] [G loss: 4.051906]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1455 [D loss: 0.801761, acc.: 60.94%] [G loss: 3.888795]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1456 [D loss: 0.159688, acc.: 96.88%] [G loss: 4.593514]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1457 [D loss: 0.132482, acc.: 98.44%] [G loss: 4.462628]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1458 [D loss: 0.146762, acc.: 93.75%] [G loss: 4.006453]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1459 [D loss: 0.296392, acc.: 90.62%] [G loss: 3.453670]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1460 [D loss: 0.115325, acc.: 98.44%] [G loss: 5.057865]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1461 [D loss: 0.102407, acc.: 98.44%] [G loss: 3.126111]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1462 [D loss: 0.114482, acc.: 98.44%] [G loss: 4.199245]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1463 [D loss: 0.059295, acc.: 100.00%] [G loss: 4.219814]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1464 [D loss: 0.097483, acc.: 98.44%] [G loss: 4.824555]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1465 [D loss: 0.937166, acc.: 46.88%] [G loss: 4.489074]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1466 [D loss: 0.121842, acc.: 96.88%] [G loss: 5.992068]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1467 [D loss: 0.616852, acc.: 71.88%] [G loss: 3.138009]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1468 [D loss: 0.232662, acc.: 87.50%] [G loss: 4.567109]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1469 [D loss: 0.089680, acc.: 98.44%] [G loss: 4.643699]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1470 [D loss: 0.153145, acc.: 93.75%] [G loss: 4.388359]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1471 [D loss: 0.236104, acc.: 90.62%] [G loss: 3.413604]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1472 [D loss: 0.061592, acc.: 100.00%] [G loss: 5.302447]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1473 [D loss: 0.243546, acc.: 90.62%] [G loss: 4.663833]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1474 [D loss: 0.100985, acc.: 96.88%] [G loss: 4.853416]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1475 [D loss: 0.658152, acc.: 65.62%] [G loss: 2.788075]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1476 [D loss: 0.163798, acc.: 93.75%] [G loss: 4.617003]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1477 [D loss: 0.129961, acc.: 96.88%] [G loss: 3.340383]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1478 [D loss: 0.224678, acc.: 90.62%] [G loss: 5.131741]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1479 [D loss: 0.571666, acc.: 65.62%] [G loss: 4.162956]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1480 [D loss: 0.138109, acc.: 95.31%] [G loss: 5.458217]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1481 [D loss: 0.438955, acc.: 68.75%] [G loss: 2.500692]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1482 [D loss: 0.588490, acc.: 78.12%] [G loss: 4.447862]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1483 [D loss: 0.210817, acc.: 93.75%] [G loss: 4.713596]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1484 [D loss: 0.095744, acc.: 98.44%] [G loss: 4.394845]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1485 [D loss: 0.154423, acc.: 96.88%] [G loss: 3.797757]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1486 [D loss: 0.669683, acc.: 75.00%] [G loss: 4.451826]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1487 [D loss: 0.161683, acc.: 93.75%] [G loss: 4.769578]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1488 [D loss: 0.444087, acc.: 76.56%] [G loss: 4.114788]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1489 [D loss: 0.128040, acc.: 98.44%] [G loss: 5.102324]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1490 [D loss: 0.145175, acc.: 95.31%] [G loss: 5.384594]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1491 [D loss: 0.797652, acc.: 59.38%] [G loss: 2.907624]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1492 [D loss: 0.200215, acc.: 89.06%] [G loss: 4.949734]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1493 [D loss: 0.170676, acc.: 95.31%] [G loss: 4.293548]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1494 [D loss: 1.156832, acc.: 45.31%] [G loss: 3.542964]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1495 [D loss: 0.260270, acc.: 84.38%] [G loss: 5.375273]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1496 [D loss: 0.299553, acc.: 89.06%] [G loss: 3.790069]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1497 [D loss: 0.230602, acc.: 95.31%] [G loss: 3.282305]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1498 [D loss: 0.117119, acc.: 98.44%] [G loss: 4.411129]\n",
      "(32, 100, 128, 1)\n",
      "(32, 1)\n",
      "1499 [D loss: 0.125300, acc.: 95.31%] [G loss: 3.865782]\n",
      "The End\n"
     ]
    }
   ],
   "source": [
    "#if __name__ == '__main__':\n",
    "print(\"hello\")\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "print(\"it's begin\")\n",
    "dcgan = DCGAN()\n",
    "\n",
    "dcgan.train(X_train_v3, epochs=1500, batch_size=32, save_interval=50)\n",
    "print(\"The End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 100, 128, 1)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "C:\\Users\\Elisa Pigeon\\Pictures\\V2\\mnist_HH3.png\n",
      "(4, 100, 128)\n",
      "(400, 128)\n",
      "(4, 100, 128)\n",
      "(100, 128)\n"
     ]
    }
   ],
   "source": [
    "r =4\n",
    "c=1\n",
    "noise = np.random.normal(0, 1, (r * c, dcgan.latent_dim))\n",
    "gen_images = dcgan.generator.predict(noise)\n",
    "\n",
    "gen_imgs = gen_images\n",
    "#gen_imgs = X_train_v3\n",
    "print(gen_imgs.shape)\n",
    "gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "gen_imgs[0].shape\n",
    "fig, axs = plt.subplots(r, c)\n",
    "cnt = 0\n",
    "plt.imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "for i in range(r):\n",
    "        print(i)\n",
    "        axs[i].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "        axs[i].axis('off')\n",
    "        cnt += 1\n",
    "str = ('C:\\\\Users\\\\Elisa Pigeon\\\\Pictures\\\\V2\\\\mnist_HH%d.png' %i)\n",
    "print(str)\n",
    "fig.savefig(str, format ='png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filename = \"C:\\\\Users\\\\Elisa Pigeon\\\\Pictures\\\\mid\\\\Felicita.mid\"\n",
    "test = pypiano.parse(filename)\n",
    "list_tracks = test.tracks\n",
    "\n",
    "\n",
    "gen_imgs = gen_images\n",
    "#gen_imgs = dcgan.generator.predict(noise)\n",
    "gen_imgs = gen_imgs[:,:,:,0]\n",
    "print(gen_imgs.shape)\n",
    "gen_big_img = gen_imgs[0]\n",
    "for i in range (1, gen_imgs.shape[0]):\n",
    "    gen_big_img = np.concatenate((gen_big_img, gen_imgs[i]), axis = 0)\n",
    "print (gen_big_img.shape)\n",
    "result_fileName = 'C:\\\\Users\\\\Elisa Pigeon\\\\Pictures\\\\Result\\\\test8.mid'\n",
    "multitrack = Multitrack()\n",
    "\n",
    "print(gen_imgs.shape)\n",
    "#gen_img = list_tracks[0]\n",
    "gen_img = gen_imgs[0]\n",
    "#print(gen_img.pianoroll.shape)\n",
    "#gen_img = np.asarray(gen_img.pianoroll)\n",
    "\n",
    "#testb = np.zeros((1000,128))\n",
    "#testb[158][0] = 1\n",
    "#print(testb.shape)\n",
    "#print (testb)\n",
    "#gen_img = testb\n",
    "print(gen_img.shape)\n",
    "gen_img = (gen_img > 0)\n",
    "#track = Track(pianoroll = gen_img, name = \"result\")\n",
    "track = Track(pianoroll = gen_big_img, name = \"result\")\n",
    "track.binarize()\n",
    "track.pianoroll\n",
    "multitrack.append_track(track)\n",
    "pypiano.write(multitrack,result_fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
